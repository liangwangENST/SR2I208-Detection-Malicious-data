{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net\n",
    "In this file, we test the neural net for all kind of attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense,Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_attack1 = pd.read_csv(\"dataset/attack1with7FeatureVector.csv\")\n",
    "data_attack1 = data_attack1.dropna(axis=0, how=\"any\")#remove invalid data\n",
    "sum(np.array(data_attack1['Label']))/data_attack1['Label'].shape[0]/1\n",
    "\n",
    "data_attack1.shape\n",
    "\n",
    "X = data_attack1.iloc[:,12:] #In our case, we use the feature of 1-6, the feature of distance rejected\n",
    "n = X.shape[1]\n",
    "y = data_attack1.iloc[:,11]\n",
    "X = np.reshape(X.values, (X.shape[0], X.shape[1]))\n",
    "y =np.reshape(y.values, (y.shape[0],  1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_func = ['sigmoid','softmax', 'elu', 'selu', 'softplus', 'softsign', \n",
    "                   'relu', 'tanh', 'hard_sigmoid', 'linear']\n",
    "loss_func = ['binary_crossentropy', 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error',\n",
    "            'mean_squared_logarithmic_error', 'squared_hinge', 'hinge', 'logcosh', 'kullback_leibler_divergence', \n",
    "            'poisson', 'cosine_proximity']\n",
    "optimizer_scheme = ['Adagrad','SGD', 'RMSprop', 'Adadelta', 'Adam', 'Adamax', 'Nadam']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********For the activation function sigmoid ********\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 1s 22us/step - loss: 0.2844 - binary_accuracy: 0.9196\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.1401 - binary_accuracy: 0.9597\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.1011 - binary_accuracy: 0.9733\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.0854 - binary_accuracy: 0.9832\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.0766 - binary_accuracy: 0.9838\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0706 - binary_accuracy: 0.9843\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.0674 - binary_accuracy: 0.9849\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.0643 - binary_accuracy: 0.9856\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.0621 - binary_accuracy: 0.9860\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0605 - binary_accuracy: 0.9865\n",
      "6118/6118 [==============================] - 0s 11us/step\n",
      "For attack1: The loss is 0.05953715152456387 , the accuracy is 0.9874141871558954\n",
      "********For the activation function softmax ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 1s 24us/step - loss: 10.9401 - binary_accuracy: 0.3138\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 10.9401 - binary_accuracy: 0.3138\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 10.9401 - binary_accuracy: 0.3138\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 10.9401 - binary_accuracy: 0.3138\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 10.9401 - binary_accuracy: 0.3138\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 10.9401 - binary_accuracy: 0.3138\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 10.9401 - binary_accuracy: 0.3138\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 10.9401 - binary_accuracy: 0.3138\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 10.9401 - binary_accuracy: 0.3138\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 10.9401 - binary_accuracy: 0.3138\n",
      "6118/6118 [==============================] - 0s 13us/step\n",
      "For attack1: The loss is 10.894918015908088 , the accuracy is 0.3166067344509491\n",
      "********For the activation function elu ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 1s 22us/step - loss: 5.0633 - binary_accuracy: 0.4222\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 5.0640 - binary_accuracy: 0.4224\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 5.0640 - binary_accuracy: 0.4224\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 5.0640 - binary_accuracy: 0.4224\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 5.0640 - binary_accuracy: 0.4224\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 5.0640 - binary_accuracy: 0.4224\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 5.0640 - binary_accuracy: 0.4224\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 5.0640 - binary_accuracy: 0.4224\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 5.0640 - binary_accuracy: 0.4224\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 5.0640 - binary_accuracy: 0.4224\n",
      "6118/6118 [==============================] - 0s 15us/step\n",
      "For attack1: The loss is 5.076752215750508 , the accuracy is 0.4171297810910846\n",
      "********For the activation function selu ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 1s 26us/step - loss: 1.4633 - binary_accuracy: 0.1383\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 3.5710 - binary_accuracy: 0.1148\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 2.4333 - binary_accuracy: 0.1061\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 1.4234 - binary_accuracy: 0.1023\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 1.0729 - binary_accuracy: 0.0641\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.9569 - binary_accuracy: 0.0710\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 1.0560 - binary_accuracy: 0.1058\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.8207 - binary_accuracy: 0.0915\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.9044 - binary_accuracy: 0.0902\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.7711 - binary_accuracy: 0.0863\n",
      "6118/6118 [==============================] - 0s 18us/step\n",
      "For attack1: The loss is 0.7592870341909208 , the accuracy is 0.08826413866340743\n",
      "********For the activation function softplus ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 1s 25us/step - loss: 10.9649 - binary_accuracy: 0.0327\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 10.9649 - binary_accuracy: 0.0327\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 10.9649 - binary_accuracy: 0.0327\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 10.9649 - binary_accuracy: 0.0327\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 10.9649 - binary_accuracy: 0.0327\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 10.9649 - binary_accuracy: 0.0327\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 10.9649 - binary_accuracy: 0.0327\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 10.9649 - binary_accuracy: 0.0327\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 10.9649 - binary_accuracy: 0.0327\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 10.9649 - binary_accuracy: 0.0327\n",
      "6118/6118 [==============================] - 0s 19us/step\n",
      "For attack1: The loss is 10.795897007456166 , the accuracy is 0.031055900634513956\n",
      "********For the activation function softsign ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 1s 27us/step - loss: 0.2613 - binary_accuracy: 0.9328\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.2482 - binary_accuracy: 0.9657\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.2341 - binary_accuracy: 0.9698\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.2208 - binary_accuracy: 0.8495\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.2137 - binary_accuracy: 0.5867\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.2216 - binary_accuracy: 0.4428\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.1975 - binary_accuracy: 0.4272\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.1936 - binary_accuracy: 0.4081\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.1867 - binary_accuracy: 0.3954\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.1806 - binary_accuracy: 0.3935\n",
      "6118/6118 [==============================] - 0s 20us/step\n",
      "For attack1: The loss is 0.15385989487492677 , the accuracy is 0.39228506065264523\n",
      "********For the activation function relu ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 1s 27us/step - loss: 3.1309 - binary_accuracy: 0.7095\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 1.6181 - binary_accuracy: 0.7106\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 3.9722 - binary_accuracy: 0.7184\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 3.7211 - binary_accuracy: 0.7219\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 3.6079 - binary_accuracy: 0.7251\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 1.6776 - binary_accuracy: 0.6798\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.3280 - binary_accuracy: 0.7021\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.5369 - binary_accuracy: 0.6751\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.4163 - binary_accuracy: 0.6827\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.1291 - binary_accuracy: 0.7182\n",
      "6118/6118 [==============================] - 0s 21us/step\n",
      "For attack1: The loss is 0.15943992762403497 , the accuracy is 0.7257273617270882\n",
      "********For the activation function tanh ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 1s 27us/step - loss: 0.2890 - binary_accuracy: 0.9419\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.1665 - binary_accuracy: 0.9544\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.1097 - binary_accuracy: 0.9700\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.1045 - binary_accuracy: 0.9710\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0937 - binary_accuracy: 0.9750\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0875 - binary_accuracy: 0.9780\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0823 - binary_accuracy: 0.9809\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0777 - binary_accuracy: 0.9830\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0733 - binary_accuracy: 0.9840\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0736 - binary_accuracy: 0.9833\n",
      "6118/6118 [==============================] - 0s 24us/step\n",
      "For attack1: The loss is 0.09947435511808919 , the accuracy is 0.9762994437951907\n",
      "********For the activation function hard_sigmoid ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 1s 34us/step - loss: 0.2394 - binary_accuracy: 0.9122\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.1285 - binary_accuracy: 0.9480\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.1149 - binary_accuracy: 0.9616\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.1203 - binary_accuracy: 0.9568\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.1092 - binary_accuracy: 0.9755\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.1074 - binary_accuracy: 0.9758\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.1038 - binary_accuracy: 0.9763\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.1014 - binary_accuracy: 0.9765\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.0997 - binary_accuracy: 0.9766\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0978 - binary_accuracy: 0.9768\n",
      "6118/6118 [==============================] - 0s 28us/step\n",
      "For attack1: The loss is 0.08546279565634857 , the accuracy is 0.9764628959037229\n",
      "********For the activation function linear ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 1s 30us/step - loss: 4.6465 - binary_accuracy: 0.3025\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 4.6020 - binary_accuracy: 0.3079\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 4.6353 - binary_accuracy: 0.1926\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 4.6264 - binary_accuracy: 0.2215\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 4.6186 - binary_accuracy: 0.2504\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 4.5727 - binary_accuracy: 0.2752\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 4.5304 - binary_accuracy: 0.2679\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 4.5954 - binary_accuracy: 0.2150\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 4.2580 - binary_accuracy: 0.2738\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 4.3722 - binary_accuracy: 0.0817\n",
      "6118/6118 [==============================] - 0s 29us/step\n",
      "For attack1: The loss is 4.6644662506013725 , the accuracy is 0.04740111148773396\n"
     ]
    }
   ],
   "source": [
    "activation_func_use = activation_func[0]\n",
    "loss_func_use = loss_func[0]\n",
    "optimizer_scheme_use = optimizer_scheme[0]\n",
    "\n",
    "for i in range(len(activation_func)):\n",
    "    activation_func_use = activation_func[i]\n",
    "    print('********For the activation function', activation_func[i], '********')\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=n, activation=activation_func_use))\n",
    "    model.add(Dense(16, activation=activation_func_use))\n",
    "    model.add(Dense(1, activation=activation_func_use))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, )\n",
    "\n",
    "    model.compile(loss=loss_func_use, optimizer=optimizer_scheme_use, metrics=['binary_accuracy'])\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=100)\n",
    "\n",
    "    loss_and_metrics = model.evaluate(X_test, y_test, batch_size=128)\n",
    "\n",
    "    print('For attack1: The loss is', loss_and_metrics[0], ', the accuracy is', loss_and_metrics[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that only the sigmoid function and hard_sigmoid function are valid for this kind of data set.\n",
    "We thus choose the Sigmoid funcion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********For the loss function binary_crossentropy ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 1s 32us/step - loss: 0.2705 - binary_accuracy: 0.9187\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.1319 - binary_accuracy: 0.9615\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0989 - binary_accuracy: 0.9699\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0840 - binary_accuracy: 0.9831\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0756 - binary_accuracy: 0.9841\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0702 - binary_accuracy: 0.9846\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0665 - binary_accuracy: 0.9848\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.0637 - binary_accuracy: 0.9860\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.0617 - binary_accuracy: 0.9861\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0602 - binary_accuracy: 0.9865\n",
      "6118/6118 [==============================] - 0s 31us/step\n",
      "For attack1: The loss is 0.061039778408504615 , the accuracy is 0.9856162144686514\n",
      "********For the loss function mean_squared_error ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 1s 30us/step - loss: 0.0679 - binary_accuracy: 0.9275\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0322 - binary_accuracy: 0.9649\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 10us/step - loss: 0.0243 - binary_accuracy: 0.9792\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0208 - binary_accuracy: 0.9832\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0186 - binary_accuracy: 0.9838\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0171 - binary_accuracy: 0.9842\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0159 - binary_accuracy: 0.9856\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0151 - binary_accuracy: 0.9855\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0146 - binary_accuracy: 0.9863\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0142 - binary_accuracy: 0.9862\n",
      "6118/6118 [==============================] - 0s 31us/step\n",
      "For attack1: The loss is 0.012328286370583398 , the accuracy is 0.9888852561326852\n",
      "********For the loss function mean_absolute_error ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 1s 31us/step - loss: 0.2348 - binary_accuracy: 0.7949\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 10us/step - loss: 0.0910 - binary_accuracy: 0.9384\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0647 - binary_accuracy: 0.9586\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0498 - binary_accuracy: 0.9702\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0422 - binary_accuracy: 0.9726\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0357 - binary_accuracy: 0.9779\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0299 - binary_accuracy: 0.9866\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0259 - binary_accuracy: 0.9877\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0237 - binary_accuracy: 0.9874\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0215 - binary_accuracy: 0.9879\n",
      "6118/6118 [==============================] - 0s 34us/step\n",
      "For attack1: The loss is 0.02118019518406222 , the accuracy is 0.987904543500977\n",
      "********For the loss function mean_absolute_percentage_error ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 1s 33us/step - loss: 32869756.8165 - binary_accuracy: 0.6824\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 9827984.9209 - binary_accuracy: 0.6832\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 6517672.7297 - binary_accuracy: 0.6832\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 4929355.5958 - binary_accuracy: 0.6832\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 3979822.4090 - binary_accuracy: 0.6832\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 3343513.2466 - binary_accuracy: 0.6832\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 2885824.7620 - binary_accuracy: 0.6832\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 2540129.1821 - binary_accuracy: 0.6832\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 2269491.2881 - binary_accuracy: 0.6832\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 2051627.7117 - binary_accuracy: 0.6832\n",
      "6118/6118 [==============================] - 0s 35us/step\n",
      "For attack1: The loss is 1991757.0711833932 , the accuracy is 0.6956521742248036\n",
      "********For the loss function mean_squared_logarithmic_error ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 1s 34us/step - loss: 0.0352 - binary_accuracy: 0.9162\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0166 - binary_accuracy: 0.9591\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0125 - binary_accuracy: 0.9683\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0104 - binary_accuracy: 0.9726\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0091 - binary_accuracy: 0.9823\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0085 - binary_accuracy: 0.9842\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0077 - binary_accuracy: 0.9850\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0073 - binary_accuracy: 0.9853\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0071 - binary_accuracy: 0.9852\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0067 - binary_accuracy: 0.9863\n",
      "6118/6118 [==============================] - 0s 38us/step\n",
      "For attack1: The loss is 0.0069993938576343616 , the accuracy is 0.9856162139620411\n",
      "********For the loss function squared_hinge ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 1s 35us/step - loss: 0.6840 - binary_accuracy: 0.3160\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.6840 - binary_accuracy: 0.3160\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.6840 - binary_accuracy: 0.3160\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.6840 - binary_accuracy: 0.3160\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.6840 - binary_accuracy: 0.3160\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.6840 - binary_accuracy: 0.3160\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.6840 - binary_accuracy: 0.3160\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.6840 - binary_accuracy: 0.3160\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.6840 - binary_accuracy: 0.3160\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.6840 - binary_accuracy: 0.3160\n",
      "6118/6118 [==============================] - 0s 38us/step\n",
      "For attack1: The loss is 0.6923845593700927 , the accuracy is 0.30761686838425306\n",
      "********For the loss function hinge ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 1s 36us/step - loss: 0.7060 - binary_accuracy: 0.3170\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.6920 - binary_accuracy: 0.3140\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.6898 - binary_accuracy: 0.3140\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.6888 - binary_accuracy: 0.3140\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.6883 - binary_accuracy: 0.3140\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.6879 - binary_accuracy: 0.3140\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.6876 - binary_accuracy: 0.3140\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.6874 - binary_accuracy: 0.3140\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.6873 - binary_accuracy: 0.3140\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.6872 - binary_accuracy: 0.3140\n",
      "6118/6118 [==============================] - 0s 39us/step\n",
      "For attack1: The loss is 0.6852915169718063 , the accuracy is 0.3157894738400906\n",
      "********For the loss function logcosh ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 1s 37us/step - loss: 0.0332 - binary_accuracy: 0.9233\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0158 - binary_accuracy: 0.9639\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0118 - binary_accuracy: 0.9720\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0098 - binary_accuracy: 0.9823\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 10us/step - loss: 0.0085 - binary_accuracy: 0.9843\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0077 - binary_accuracy: 0.9850\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0071 - binary_accuracy: 0.9859\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0067 - binary_accuracy: 0.9863\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0065 - binary_accuracy: 0.9865\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0062 - binary_accuracy: 0.9872\n",
      "6118/6118 [==============================] - 0s 41us/step\n",
      "For attack1: The loss is 0.0051992954975514675 , the accuracy is 0.989539064566814\n",
      "********For the loss function kullback_leibler_divergence ********\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1238: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 1s 38us/step - loss: 0.0137 - binary_accuracy: 0.3167\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0042 - binary_accuracy: 0.3157\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0029 - binary_accuracy: 0.3157\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0022 - binary_accuracy: 0.3157\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0018 - binary_accuracy: 0.3157\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0015 - binary_accuracy: 0.3157\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0013 - binary_accuracy: 0.3157\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0012 - binary_accuracy: 0.3157\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0011 - binary_accuracy: 0.3157\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 9.5548e-04 - binary_accuracy: 0.3157\n",
      "6118/6118 [==============================] - 0s 43us/step\n",
      "For attack1: The loss is 0.0008927017078747417 , the accuracy is 0.3090879372490041\n",
      "********For the loss function poisson ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 1s 39us/step - loss: 0.4946 - binary_accuracy: 0.9084\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.4102 - binary_accuracy: 0.9524\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 10us/step - loss: 0.3839 - binary_accuracy: 0.9662\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.3733 - binary_accuracy: 0.9702\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.3679 - binary_accuracy: 0.9803\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.3650 - binary_accuracy: 0.9828\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.3623 - binary_accuracy: 0.9835\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.3610 - binary_accuracy: 0.9837\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.3595 - binary_accuracy: 0.9842\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 10us/step - loss: 0.3587 - binary_accuracy: 0.9845\n",
      "6118/6118 [==============================] - 0s 46us/step\n",
      "For attack1: The loss is 0.35119265609217765 , the accuracy is 0.984472049689441\n",
      "********For the loss function cosine_proximity ********\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3003: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 1s 40us/step - loss: -0.3121 - binary_accuracy: 0.3259\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: -0.3121 - binary_accuracy: 0.3121\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: -0.3121 - binary_accuracy: 0.3121\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: -0.3121 - binary_accuracy: 0.3121\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: -0.3121 - binary_accuracy: 0.3121\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: -0.3121 - binary_accuracy: 0.3121\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: -0.3121 - binary_accuracy: 0.3121\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: -0.3121 - binary_accuracy: 0.3121\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: -0.3121 - binary_accuracy: 0.3121\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: -0.3121 - binary_accuracy: 0.3121\n",
      "6118/6118 [==============================] - 0s 45us/step\n",
      "For attack1: The loss is -0.32347172295084653 , the accuracy is 0.32347172295084653\n"
     ]
    }
   ],
   "source": [
    "activation_func_use = activation_func[0]\n",
    "loss_func_use = loss_func[0]\n",
    "optimizer_scheme_use = optimizer_scheme[0]\n",
    "\n",
    "for i in range(len(loss_func)):\n",
    "    loss_func_use = loss_func[i]\n",
    "    print('********For the loss function', loss_func[i], '********')\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=n, activation=activation_func_use))\n",
    "    model.add(Dense(16, activation=activation_func_use))\n",
    "    model.add(Dense(1, activation=activation_func_use))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, )\n",
    "\n",
    "    model.compile(loss=loss_func_use, optimizer=optimizer_scheme_use, metrics=['binary_accuracy'])\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=100)\n",
    "\n",
    "    loss_and_metrics = model.evaluate(X_test, y_test, batch_size=128)\n",
    "\n",
    "    print('For attack1: The loss is', loss_and_metrics[0], ', the accuracy is', loss_and_metrics[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that many loss functions are effective for this optimization problem. \n",
    "So we choose the 'mean_absolute_error' as the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the proper Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********For the optimizer_scheme Adagrad ***********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 1s 43us/step - loss: 0.2744 - binary_accuracy: 0.9144\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.1417 - binary_accuracy: 0.9644\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.1101 - binary_accuracy: 0.9749\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0957 - binary_accuracy: 0.9826\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0858 - binary_accuracy: 0.9837\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0796 - binary_accuracy: 0.9842\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0756 - binary_accuracy: 0.9846\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0719 - binary_accuracy: 0.9857\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0697 - binary_accuracy: 0.9859\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0672 - binary_accuracy: 0.9864\n",
      "6118/6118 [==============================] - 0s 48us/step\n",
      "For attack1: The loss is 0.07421896336085658 , the accuracy is 0.9843085976003938\n",
      "***********For the optimizer_scheme SGD ***********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 1s 43us/step - loss: 0.5650 - binary_accuracy: 0.6975\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.4864 - binary_accuracy: 0.7246\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.4343 - binary_accuracy: 0.8961\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.3922 - binary_accuracy: 0.9023\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.3582 - binary_accuracy: 0.9057\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.3321 - binary_accuracy: 0.9120\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.3098 - binary_accuracy: 0.9183\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.2927 - binary_accuracy: 0.9237\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.2795 - binary_accuracy: 0.9270\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.2687 - binary_accuracy: 0.9281\n",
      "6118/6118 [==============================] - 0s 52us/step\n",
      "For attack1: The loss is 0.2580100839416277 , the accuracy is 0.9311866618792736\n",
      "***********For the optimizer_scheme RMSprop ***********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 1s 46us/step - loss: 0.3334 - binary_accuracy: 0.8821\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.1173 - binary_accuracy: 0.9624\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0752 - binary_accuracy: 0.9819\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0640 - binary_accuracy: 0.9845\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0604 - binary_accuracy: 0.9851\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0570 - binary_accuracy: 0.9861\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0552 - binary_accuracy: 0.9865\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0529 - binary_accuracy: 0.9870\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0507 - binary_accuracy: 0.9873\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0482 - binary_accuracy: 0.9875\n",
      "6118/6118 [==============================] - 0s 53us/step\n",
      "For attack1: The loss is 0.041415153970639274 , the accuracy is 0.988721804043638\n",
      "***********For the optimizer_scheme Adadelta ***********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 1s 50us/step - loss: 0.5297 - binary_accuracy: 0.8636\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.3204 - binary_accuracy: 0.9294\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.2231 - binary_accuracy: 0.9360\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.1585 - binary_accuracy: 0.9534\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.1085 - binary_accuracy: 0.9700\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.0851 - binary_accuracy: 0.9790\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.0728 - binary_accuracy: 0.9848\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.0656 - binary_accuracy: 0.9853\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.0609 - binary_accuracy: 0.9860\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.0583 - binary_accuracy: 0.9864\n",
      "6118/6118 [==============================] - 0s 56us/step\n",
      "For attack1: The loss is 0.06248773655928148 , the accuracy is 0.985289310232102\n",
      "***********For the optimizer_scheme Adam ***********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 1s 52us/step - loss: 0.3384 - binary_accuracy: 0.8917\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.1265 - binary_accuracy: 0.9640\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.0831 - binary_accuracy: 0.9819\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.0721 - binary_accuracy: 0.9828\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.0642 - binary_accuracy: 0.9849\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.0604 - binary_accuracy: 0.9861\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.0612 - binary_accuracy: 0.9856\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.0561 - binary_accuracy: 0.9868\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.0549 - binary_accuracy: 0.9867\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.0527 - binary_accuracy: 0.9875\n",
      "6118/6118 [==============================] - 0s 59us/step\n",
      "For attack1: The loss is 0.04824050949431588 , the accuracy is 0.9887218045112782\n",
      "***********For the optimizer_scheme Adamax ***********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 1s 52us/step - loss: 0.3823 - binary_accuracy: 0.8729\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.1814 - binary_accuracy: 0.9477\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.1048 - binary_accuracy: 0.9729\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0787 - binary_accuracy: 0.9832\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0669 - binary_accuracy: 0.9844\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.0608 - binary_accuracy: 0.9862\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0577 - binary_accuracy: 0.9866\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.0557 - binary_accuracy: 0.9870\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0544 - binary_accuracy: 0.9873\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0537 - binary_accuracy: 0.9874\n",
      "6118/6118 [==============================] - 0s 59us/step\n",
      "For attack1: The loss is 0.05129734783799635 , the accuracy is 0.9874141871558954\n",
      "***********For the optimizer_scheme Nadam ***********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 1s 56us/step - loss: 0.2412 - binary_accuracy: 0.9228\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.0848 - binary_accuracy: 0.9781\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.0638 - binary_accuracy: 0.9847\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.0639 - binary_accuracy: 0.9844\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.0583 - binary_accuracy: 0.9861\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.0537 - binary_accuracy: 0.9868\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.0550 - binary_accuracy: 0.9859\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.0514 - binary_accuracy: 0.9869\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.0467 - binary_accuracy: 0.9879\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.0447 - binary_accuracy: 0.9881\n",
      "6118/6118 [==============================] - 0s 66us/step\n",
      "For attack1: The loss is 0.0400968008151326 , the accuracy is 0.9897025171624714\n"
     ]
    }
   ],
   "source": [
    "activation_func_use = activation_func[0]\n",
    "loss_func_use = loss_func[0]\n",
    "optimizer_scheme_use = optimizer_scheme[0]\n",
    "\n",
    "for i in range(len(optimizer_scheme)):\n",
    "    optimizer_scheme_use = optimizer_scheme[i]\n",
    "    print('***********For the optimizer_scheme', optimizer_scheme[i], '***********')\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=n, activation=activation_func_use))\n",
    "    model.add(Dense(16, activation=activation_func_use))\n",
    "    model.add(Dense(1, activation=activation_func_use))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, )\n",
    "\n",
    "    model.compile(loss=loss_func_use, optimizer=optimizer_scheme_use, metrics=['binary_accuracy'])\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=100)\n",
    "\n",
    "    loss_and_metrics = model.evaluate(X_test, y_test, batch_size=128)\n",
    "\n",
    "    print('For attack1: The loss is', loss_and_metrics[0], ', the accuracy is', loss_and_metrics[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that besides from SGD, the others are quite efficient. \n",
    "\n",
    "Considering the setup speed, we choose the RMSprop as the Optimizer scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_func_use = activation_func[0] #'sigmoid'\n",
    "loss_func_use = loss_func[2] #'mean_absolute_error'\n",
    "optimizer_scheme_use = optimizer_scheme[2] #'RMSprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_predict, y_test):\n",
    "    # Our evaluation function. \n",
    "    count_ccr = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_predict[i]==y_test[i]:\n",
    "            count_ccr+=1\n",
    "        if y_predict[i]==1 and y_test[i]==1:\n",
    "            TP+=1\n",
    "        if y_predict[i]==1 and y_test[i]==0:\n",
    "            FP+=1\n",
    "        if y_predict[i]==0 and y_test[i]==1:\n",
    "            FN+=1\n",
    "    ccr = count_ccr/len(y_test)\n",
    "    if (TP+FP)==0:\n",
    "        print('All the prediction is normal')\n",
    "        preci = 0\n",
    "    else:\n",
    "        preci = TP/(TP+FP)\n",
    "    recall= TP/(TP+FN)\n",
    "    print('For this model, the CCR is', ccr, ', the Precision is', preci, 'and the Recall is', recall )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(y, y_train, y_test, y_predict):\n",
    "    # displays the statistics\n",
    "    print('There are ', len(y), 'session in total')\n",
    "    \n",
    "    malicious_train = sum(y_train)/len(y_train)\n",
    "    normal_train = 1-malicious_train\n",
    "    print('The training dataset has,', len(y_train), 'sessions, there are ', malicious_train, 'data, and', normal_train, 'normal data')\n",
    "    \n",
    "    malicious_test = sum(y_test)/len(y_test)\n",
    "    normal_test = 1-malicious_test\n",
    "    print('The testing dataset has',len(y_test), 'sessions, and there are ', malicious_test, 'data, and', normal_test, 'normal data')\n",
    "    \n",
    "    malicious_predict = sum(y_predict)/len(y_predict)\n",
    "    normal_predict = 1-malicious_predict\n",
    "    print('The prediction includes ', malicious_predict, 'data, and', normal_predict, 'normal data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "24470/24470 [==============================] - 0s 17us/step - loss: 0.2308 - binary_accuracy: 0.8276\n",
      "Epoch 2/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0752 - binary_accuracy: 0.9377\n",
      "Epoch 3/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0478 - binary_accuracy: 0.9590\n",
      "Epoch 4/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0280 - binary_accuracy: 0.9794\n",
      "Epoch 5/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0177 - binary_accuracy: 0.9864\n",
      "Epoch 6/20\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0145 - binary_accuracy: 0.9872\n",
      "Epoch 7/20\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0137 - binary_accuracy: 0.9871\n",
      "Epoch 8/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0126 - binary_accuracy: 0.9878\n",
      "Epoch 9/20\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0128 - binary_accuracy: 0.9876\n",
      "Epoch 10/20\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0124 - binary_accuracy: 0.9879\n",
      "Epoch 11/20\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0120 - binary_accuracy: 0.9882\n",
      "Epoch 12/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0122 - binary_accuracy: 0.9881\n",
      "Epoch 13/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0117 - binary_accuracy: 0.9884\n",
      "Epoch 14/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0117 - binary_accuracy: 0.9883\n",
      "Epoch 15/20\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0117 - binary_accuracy: 0.9883\n",
      "Epoch 16/20\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0119 - binary_accuracy: 0.9881\n",
      "Epoch 17/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0118 - binary_accuracy: 0.9883\n",
      "Epoch 18/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0114 - binary_accuracy: 0.9886\n",
      "Epoch 19/20\n",
      "24470/24470 [==============================] - 0s 11us/step - loss: 0.0113 - binary_accuracy: 0.9888\n",
      "Epoch 20/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0113 - binary_accuracy: 0.9888\n",
      "For this model, the CCR is 0.9898659692710036 , the Precision is 1.0 and the Recall is 0.9686075949367089\n",
      "There are  30588 session in total\n",
      "The training dataset has, 24470 sessions, there are  [0.31221904] data, and [0.68778096] normal data\n",
      "The testing dataset has 6118 sessions, and there are  [0.32281791] data, and [0.67718209] normal data\n",
      "The prediction includes  [0.31268388] data, and [0.6873161] normal data\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=n, activation=activation_func_use))\n",
    "model.add(Dense(16, activation=activation_func_use))\n",
    "model.add(Dense(1, activation=activation_func_use))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, )\n",
    "\n",
    "model.compile(loss=loss_func_use, optimizer=optimizer_scheme_use, metrics=['binary_accuracy'])\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=100)\n",
    "y_predict = np.round(model.predict(X_test))\n",
    "evaluate_model(y_predict, y_test)\n",
    "stats(y,y_train, y_test, y_predict)\n",
    "#print('For attack1: The loss is', loss_and_metrics[0], ', the accuracy is', loss_and_metrics[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3143389564535112"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_attack2 = pd.read_csv(\"dataset/attack2with7FeatureVector.csv\")\n",
    "data_attack2 = data_attack2.dropna(axis=0, how=\"any\")#remove invalid data\n",
    "data_attack2['Label'] = data_attack2['Label']/2\n",
    "sum(np.array(data_attack2['Label']))/data_attack2['Label'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "24470/24470 [==============================] - 1s 61us/step - loss: 0.3314 - binary_accuracy: 0.6857\n",
      "Epoch 2/20\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.3149 - binary_accuracy: 0.6857\n",
      "Epoch 3/20\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.3143 - binary_accuracy: 0.6857\n",
      "Epoch 4/20\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.3143 - binary_accuracy: 0.6857\n",
      "Epoch 5/20\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.3143 - binary_accuracy: 0.6857\n",
      "Epoch 6/20\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.3143 - binary_accuracy: 0.6857\n",
      "Epoch 7/20\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.3143 - binary_accuracy: 0.6857\n",
      "Epoch 8/20\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.3143 - binary_accuracy: 0.6857\n",
      "Epoch 9/20\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.3143 - binary_accuracy: 0.6857\n",
      "Epoch 10/20\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.3143 - binary_accuracy: 0.6857\n",
      "Epoch 11/20\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.3143 - binary_accuracy: 0.6857\n",
      "Epoch 12/20\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.3143 - binary_accuracy: 0.6857\n",
      "Epoch 13/20\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.3143 - binary_accuracy: 0.6857\n",
      "Epoch 14/20\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.3143 - binary_accuracy: 0.6857\n",
      "Epoch 15/20\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.3143 - binary_accuracy: 0.6857\n",
      "Epoch 16/20\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.3143 - binary_accuracy: 0.6857\n",
      "Epoch 17/20\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.3143 - binary_accuracy: 0.6857\n",
      "Epoch 18/20\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.3143 - binary_accuracy: 0.6857\n",
      "Epoch 19/20\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.3143 - binary_accuracy: 0.6857\n",
      "Epoch 20/20\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.3143 - binary_accuracy: 0.6857\n",
      "All the prediction is normal\n",
      "For this model, the CCR is 0.6853546910755148 , the Precision is 0 and the Recall is 0.0\n",
      "There are  30588 session in total\n",
      "The training dataset has, 24470 sessions, there are  [0.31426236] data, and [0.68573764] normal data\n",
      "The testing dataset has 6118 sessions, and there are  [0.31464531] data, and [0.68535469] normal data\n",
      "The prediction includes  [0.] data, and [1.] normal data\n"
     ]
    }
   ],
   "source": [
    "X = data_attack2.iloc[:,12:] \n",
    "n = X.shape[1]\n",
    "y = data_attack2.iloc[:,11]\n",
    "X = np.reshape(X.values, (X.shape[0], X.shape[1]))\n",
    "y = np.reshape(y.values, (y.shape[0],  1))\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(64, input_dim=n, activation=activation_func_use))\n",
    "model2.add(Dense(16, activation=activation_func_use))\n",
    "model2.add(Dense(1, activation=activation_func_use))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, )\n",
    "\n",
    "model2.compile(loss=loss_func_use, optimizer=optimizer_scheme_use, metrics=['binary_accuracy'])\n",
    "model2.fit(X_train, y_train, epochs=20, batch_size=100)\n",
    "y_predict = np.round(model2.predict(X_test))\n",
    "\n",
    "evaluate_model(y_predict, y_test)\n",
    "stats(y,y_train, y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3448534690947066"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_attack4 = pd.read_csv(\"dataset/attack4with7FeatureVector.csv\")\n",
    "data_attack4 = data_attack4.dropna(axis=0, how=\"any\")#remove invalid data\n",
    "data_attack4['Label'] = data_attack4['Label']/4\n",
    "sum(np.array(data_attack4['Label']))/data_attack4['Label'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "24513/24513 [==============================] - 1s 55us/step - loss: 0.2839 - binary_accuracy: 0.7803\n",
      "Epoch 2/10\n",
      "24513/24513 [==============================] - 0s 13us/step - loss: 0.0838 - binary_accuracy: 0.9388\n",
      "Epoch 3/10\n",
      "24513/24513 [==============================] - 0s 14us/step - loss: 0.0422 - binary_accuracy: 0.9624\n",
      "Epoch 4/10\n",
      "24513/24513 [==============================] - 0s 14us/step - loss: 0.0382 - binary_accuracy: 0.9630\n",
      "Epoch 5/10\n",
      "24513/24513 [==============================] - 0s 13us/step - loss: 0.0380 - binary_accuracy: 0.9629\n",
      "Epoch 6/10\n",
      "24513/24513 [==============================] - 0s 14us/step - loss: 0.0374 - binary_accuracy: 0.9630\n",
      "Epoch 7/10\n",
      "24513/24513 [==============================] - 0s 15us/step - loss: 0.0377 - binary_accuracy: 0.9626\n",
      "Epoch 8/10\n",
      "24513/24513 [==============================] - 0s 13us/step - loss: 0.0378 - binary_accuracy: 0.9625\n",
      "Epoch 9/10\n",
      "24513/24513 [==============================] - 0s 13us/step - loss: 0.0368 - binary_accuracy: 0.9635\n",
      "Epoch 10/10\n",
      "24513/24513 [==============================] - 0s 13us/step - loss: 0.0212 - binary_accuracy: 0.9792\n",
      "For this model, the CCR is 0.9825420133790178 , the Precision is 0.9989949748743718 and the Recall is 0.9498327759197325\n",
      "In the training dataset, there are  [0.34569412] data, and [0.65430588] normal data\n",
      "In the testing dataset, there are  [0.34149127] data, and [0.65850873] normal data\n",
      "The prediction includes  [0.32468593] data, and [0.67531407] normal data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = data_attack4.iloc[:,12:] \n",
    "n = X.shape[1]\n",
    "y = data_attack4.iloc[:,11]\n",
    "X = np.reshape(X.values, (X.shape[0], X.shape[1]))\n",
    "y =np.reshape(y.values, (y.shape[0],  1))\n",
    "\n",
    "model4 = Sequential()\n",
    "model4.add(Dense(64, input_dim=n, activation=activation_func_use))\n",
    "model4.add(Dense(16, activation=activation_func_use))\n",
    "model4.add(Dense(1, activation=activation_func_use))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, )\n",
    "\n",
    "model4.compile(loss=loss_func_use, optimizer=optimizer_scheme_use, metrics=['binary_accuracy'])\n",
    "model4.fit(X_train, y_train, epochs=10, batch_size=100)\n",
    "y_predict = np.round(model4.predict(X_test))\n",
    "\n",
    "evaluate_model(y_predict, y_test)\n",
    "stats(y,y_train, y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2550987057131651"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_attack8 = pd.read_csv(\"dataset/attack8with7FeatureVector.csv\")\n",
    "data_attack8 = data_attack8.dropna(axis=0, how=\"any\")#remove invalid data\n",
    "data_attack8['Label'] = data_attack8['Label']/8\n",
    "sum(np.array(data_attack8['Label']))/data_attack8['Label'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "24476/24476 [==============================] - 1s 58us/step - loss: 0.2295 - binary_accuracy: 0.8396\n",
      "Epoch 2/10\n",
      "24476/24476 [==============================] - 0s 14us/step - loss: 0.0653 - binary_accuracy: 0.9535\n",
      "Epoch 3/10\n",
      "24476/24476 [==============================] - 0s 13us/step - loss: 0.0399 - binary_accuracy: 0.9660\n",
      "Epoch 4/10\n",
      "24476/24476 [==============================] - 0s 15us/step - loss: 0.0328 - binary_accuracy: 0.9694\n",
      "Epoch 5/10\n",
      "24476/24476 [==============================] - 0s 13us/step - loss: 0.0308 - binary_accuracy: 0.9705\n",
      "Epoch 6/10\n",
      "24476/24476 [==============================] - 0s 14us/step - loss: 0.0300 - binary_accuracy: 0.9708\n",
      "Epoch 7/10\n",
      "24476/24476 [==============================] - 0s 13us/step - loss: 0.0300 - binary_accuracy: 0.9707\n",
      "Epoch 8/10\n",
      "24476/24476 [==============================] - 0s 13us/step - loss: 0.0295 - binary_accuracy: 0.9712\n",
      "Epoch 9/10\n",
      "24476/24476 [==============================] - 0s 13us/step - loss: 0.0296 - binary_accuracy: 0.9709\n",
      "Epoch 10/10\n",
      "24476/24476 [==============================] - 0s 13us/step - loss: 0.0293 - binary_accuracy: 0.9713\n",
      "For this model, the CCR is 0.9725490196078431 , the Precision is 0.9954819277108434 and the Recall is 0.8908355795148248\n",
      "In the training dataset, there are  [0.25825298] data, and [0.74174702] normal data\n",
      "In the testing dataset, there are  [0.24248366] data, and [0.75751634] normal data\n",
      "The prediction includes  [0.21699347] data, and [0.78300655] normal data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = data_attack8.iloc[:,12:] \n",
    "n = X.shape[1]\n",
    "y = data_attack8.iloc[:,11]\n",
    "X = np.reshape(X.values, (X.shape[0], X.shape[1]))\n",
    "y =np.reshape(y.values, (y.shape[0],  1))\n",
    "\n",
    "model8 = Sequential()\n",
    "model8.add(Dense(64, input_dim=n, activation=activation_func_use))\n",
    "model8.add(Dense(16, activation=activation_func_use))\n",
    "model8.add(Dense(1, activation=activation_func_use))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, )\n",
    "\n",
    "model8.compile(loss=loss_func_use, optimizer=optimizer_scheme_use, metrics=['binary_accuracy'])\n",
    "model8.fit(X_train, y_train, epochs=10, batch_size=100)\n",
    "y_predict = np.round(model8.predict(X_test))\n",
    "\n",
    "evaluate_model(y_predict, y_test)\n",
    "stats(y,y_train, y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2963955426293258"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_attack16 = pd.read_csv(\"dataset/attack16with7FeatureVector.csv\")\n",
    "data_attack16 = data_attack16.dropna(axis=0, how=\"any\")#remove invalid data\n",
    "data_attack16['Label'] = data_attack16['Label']/16\n",
    "sum(np.array(data_attack16['Label']))/data_attack16['Label'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "24480/24480 [==============================] - 1s 58us/step - loss: 0.2935 - binary_accuracy: 0.7703\n",
      "Epoch 2/10\n",
      "24480/24480 [==============================] - 0s 13us/step - loss: 0.0929 - binary_accuracy: 0.9300\n",
      "Epoch 3/10\n",
      "24480/24480 [==============================] - 0s 13us/step - loss: 0.0677 - binary_accuracy: 0.9379\n",
      "Epoch 4/10\n",
      "24480/24480 [==============================] - 0s 14us/step - loss: 0.0527 - binary_accuracy: 0.9518\n",
      "Epoch 5/10\n",
      "24480/24480 [==============================] - 0s 14us/step - loss: 0.0463 - binary_accuracy: 0.9570\n",
      "Epoch 6/10\n",
      "24480/24480 [==============================] - 0s 13us/step - loss: 0.0432 - binary_accuracy: 0.9597\n",
      "Epoch 7/10\n",
      "24480/24480 [==============================] - 0s 14us/step - loss: 0.0394 - binary_accuracy: 0.9631\n",
      "Epoch 8/10\n",
      "24480/24480 [==============================] - 0s 13us/step - loss: 0.0369 - binary_accuracy: 0.9652\n",
      "Epoch 9/10\n",
      "24480/24480 [==============================] - 0s 14us/step - loss: 0.0362 - binary_accuracy: 0.9652\n",
      "Epoch 10/10\n",
      "24480/24480 [==============================] - 0s 13us/step - loss: 0.0348 - binary_accuracy: 0.9665\n",
      "For this model, the CCR is 0.9647116484234602 , the Precision is 0.985 and the Recall is 0.8914027149321267\n",
      "In the training dataset, there are  [0.29828431] data, and [0.70171569] normal data\n",
      "In the testing dataset, there are  [0.28884169] data, and [0.71115831] normal data\n",
      "The prediction includes  [0.2613952] data, and [0.7386048] normal data\n"
     ]
    }
   ],
   "source": [
    "X = data_attack16.iloc[:,12:] \n",
    "n = X.shape[1]\n",
    "y = data_attack16.iloc[:,11]\n",
    "X = np.reshape(X.values, (X.shape[0], X.shape[1]))\n",
    "y =np.reshape(y.values, (y.shape[0],  1))\n",
    "\n",
    "model16 = Sequential()\n",
    "model16.add(Dense(64, input_dim=n, activation=activation_func_use))\n",
    "model16.add(Dense(16, activation=activation_func_use))\n",
    "model16.add(Dense(1, activation=activation_func_use))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, )\n",
    "\n",
    "model16.compile(loss=loss_func_use, optimizer=optimizer_scheme_use, metrics=['binary_accuracy'])\n",
    "model16.fit(X_train, y_train, epochs=10, batch_size=100)\n",
    "y_predict = np.round(model16.predict(X_test))\n",
    "\n",
    "evaluate_model(y_predict, y_test)\n",
    "stats(y,y_train, y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Detection\n",
    "We combine all the data together and train a neural network model named 'model_all'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_overall = data_attack1.append([data_attack2, data_attack4, data_attack8, data_attack16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "122412/122412 [==============================] - 2s 16us/step - loss: 0.1644 - binary_accuracy: 0.8545\n",
      "Epoch 2/20\n",
      "122412/122412 [==============================] - 1s 7us/step - loss: 0.0871 - binary_accuracy: 0.9173\n",
      "Epoch 3/20\n",
      "122412/122412 [==============================] - 1s 7us/step - loss: 0.0800 - binary_accuracy: 0.9213\n",
      "Epoch 4/20\n",
      "122412/122412 [==============================] - 1s 7us/step - loss: 0.0785 - binary_accuracy: 0.9222\n",
      "Epoch 5/20\n",
      "122412/122412 [==============================] - 1s 7us/step - loss: 0.0781 - binary_accuracy: 0.9224\n",
      "Epoch 6/20\n",
      "122412/122412 [==============================] - 1s 7us/step - loss: 0.0775 - binary_accuracy: 0.9228\n",
      "Epoch 7/20\n",
      "122412/122412 [==============================] - 1s 7us/step - loss: 0.0775 - binary_accuracy: 0.9228\n",
      "Epoch 8/20\n",
      "122412/122412 [==============================] - 1s 7us/step - loss: 0.0773 - binary_accuracy: 0.9230\n",
      "Epoch 9/20\n",
      "122412/122412 [==============================] - 1s 7us/step - loss: 0.0770 - binary_accuracy: 0.9234\n",
      "Epoch 10/20\n",
      "122412/122412 [==============================] - 1s 7us/step - loss: 0.0771 - binary_accuracy: 0.9231\n",
      "Epoch 11/20\n",
      "122412/122412 [==============================] - 1s 7us/step - loss: 0.0768 - binary_accuracy: 0.9234\n",
      "Epoch 12/20\n",
      "122412/122412 [==============================] - 1s 7us/step - loss: 0.0769 - binary_accuracy: 0.9233\n",
      "Epoch 13/20\n",
      "122412/122412 [==============================] - 1s 7us/step - loss: 0.0769 - binary_accuracy: 0.9234\n",
      "Epoch 14/20\n",
      "122412/122412 [==============================] - 1s 7us/step - loss: 0.0767 - binary_accuracy: 0.9236\n",
      "Epoch 15/20\n",
      "122412/122412 [==============================] - 1s 7us/step - loss: 0.0766 - binary_accuracy: 0.9235\n",
      "Epoch 16/20\n",
      "122412/122412 [==============================] - 1s 7us/step - loss: 0.0767 - binary_accuracy: 0.9236\n",
      "Epoch 17/20\n",
      "122412/122412 [==============================] - 1s 7us/step - loss: 0.0767 - binary_accuracy: 0.9235\n",
      "Epoch 18/20\n",
      "122412/122412 [==============================] - 1s 7us/step - loss: 0.0768 - binary_accuracy: 0.9234\n",
      "Epoch 19/20\n",
      "122412/122412 [==============================] - 1s 7us/step - loss: 0.0763 - binary_accuracy: 0.9239\n",
      "Epoch 20/20\n",
      "122412/122412 [==============================] - 1s 7us/step - loss: 0.0766 - binary_accuracy: 0.9236\n",
      "For this model, the CCR is 0.924353821520766 , the Precision is 0.9948070175438597 and the Recall is 0.7567798419816357\n",
      "In the training dataset, there are  [0.3047577] data, and [0.6952423] normal data\n",
      "In the testing dataset, there are  [0.30604843] data, and [0.69395157] normal data\n",
      "The prediction includes  [0.23282032] data, and [0.76717967] normal data\n"
     ]
    }
   ],
   "source": [
    "X = data_overall.iloc[:,12:] \n",
    "n = X.shape[1]\n",
    "y = data_overall.iloc[:,11]\n",
    "X = np.reshape(X.values, (X.shape[0], X.shape[1]))\n",
    "y =np.reshape(y.values, (y.shape[0],  1))\n",
    "\n",
    "model_all = Sequential()\n",
    "model_all.add(Dense(64, input_dim=n, activation=activation_func_use))\n",
    "model_all.add(Dense(16, activation=activation_func_use))\n",
    "model_all.add(Dense(1, activation=activation_func_use))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, )\n",
    "\n",
    "model_all.compile(loss=loss_func_use, optimizer=optimizer_scheme_use, metrics=['binary_accuracy'])\n",
    "model_all.fit(X_train, y_train, epochs=20, batch_size=200)\n",
    "y_predict = np.round(model_all.predict(X_test))\n",
    "\n",
    "evaluate_model(y_predict, y_test)\n",
    "stats(y, y_train, y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "We first take all the malicious data, then try to classify them with the features as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "malicicous_1 = data_attack1.loc[(data_attack1['Label']==1)]\n",
    "malicicous_2 = data_attack2.loc[(data_attack2['Label']==1)]\n",
    "malicicous_4 = data_attack4.loc[(data_attack4['Label']==1)]\n",
    "malicicous_8 = data_attack8.loc[(data_attack8['Label']==1)]\n",
    "malicicous_16 = data_attack16.loc[(data_attack16['Label']==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the label.\n",
    "len1 = malicicous_1.shape[0]\n",
    "len2 = malicicous_2.shape[0]\n",
    "len4 = malicicous_4.shape[0]\n",
    "len8 = malicicous_8.shape[0]\n",
    "len16 = malicicous_16.shape[0]\n",
    "\n",
    "length = (len1 + len2 + len4 + len8 + len16)\n",
    "malicious_labels = np.zeros((length, 5))\n",
    "for i in range(length):\n",
    "    if i < len1:\n",
    "        malicious_labels[i, 0] = 1\n",
    "    if i >= len1 and i< (len1+len2):\n",
    "        malicious_labels[i, 1] = 1\n",
    "    if i >= (len1+len2) and i< (len1+len2+len4):\n",
    "        malicious_labels[i, 2] = 1\n",
    "    if i >= (len1+len2+len4) and i< (len1+len2+len4+len8):\n",
    "        malicious_labels[i, 3] = 1\n",
    "    if i >= (len1+len2+len4+len8):\n",
    "        malicious_labels[i, 4] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "malicious_data_overall = malicicous_1.append([malicicous_2, malicicous_4, malicicous_8, malicicous_16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "37337/37337 [==============================] - 1s 24us/step - loss: 1.2799 - categorical_accuracy: 0.5350\n",
      "Epoch 2/20\n",
      "37337/37337 [==============================] - 0s 13us/step - loss: 0.7822 - categorical_accuracy: 0.7402\n",
      "Epoch 3/20\n",
      "37337/37337 [==============================] - 0s 13us/step - loss: 0.5638 - categorical_accuracy: 0.7917\n",
      "Epoch 4/20\n",
      "37337/37337 [==============================] - 0s 13us/step - loss: 0.4762 - categorical_accuracy: 0.8137\n",
      "Epoch 5/20\n",
      "37337/37337 [==============================] - 0s 13us/step - loss: 0.4350 - categorical_accuracy: 0.8226\n",
      "Epoch 6/20\n",
      "37337/37337 [==============================] - 0s 13us/step - loss: 0.4092 - categorical_accuracy: 0.8274\n",
      "Epoch 7/20\n",
      "37337/37337 [==============================] - 0s 13us/step - loss: 0.3921 - categorical_accuracy: 0.8331\n",
      "Epoch 8/20\n",
      "37337/37337 [==============================] - 0s 13us/step - loss: 0.3818 - categorical_accuracy: 0.8327\n",
      "Epoch 9/20\n",
      "37337/37337 [==============================] - 0s 13us/step - loss: 0.3692 - categorical_accuracy: 0.8357\n",
      "Epoch 10/20\n",
      "37337/37337 [==============================] - 0s 13us/step - loss: 0.3626 - categorical_accuracy: 0.8389\n",
      "Epoch 11/20\n",
      "37337/37337 [==============================] - 0s 13us/step - loss: 0.3550 - categorical_accuracy: 0.8411\n",
      "Epoch 12/20\n",
      "37337/37337 [==============================] - 0s 13us/step - loss: 0.3525 - categorical_accuracy: 0.8423\n",
      "Epoch 13/20\n",
      "37337/37337 [==============================] - 0s 13us/step - loss: 0.3484 - categorical_accuracy: 0.8433\n",
      "Epoch 14/20\n",
      "37337/37337 [==============================] - 0s 13us/step - loss: 0.3445 - categorical_accuracy: 0.8434\n",
      "Epoch 15/20\n",
      "37337/37337 [==============================] - 0s 13us/step - loss: 0.3436 - categorical_accuracy: 0.8441\n",
      "Epoch 16/20\n",
      "37337/37337 [==============================] - 0s 13us/step - loss: 0.3409 - categorical_accuracy: 0.8443\n",
      "Epoch 17/20\n",
      "37337/37337 [==============================] - 0s 13us/step - loss: 0.3400 - categorical_accuracy: 0.8442\n",
      "Epoch 18/20\n",
      "37337/37337 [==============================] - 0s 13us/step - loss: 0.3385 - categorical_accuracy: 0.8469\n",
      "Epoch 19/20\n",
      "37337/37337 [==============================] - 0s 13us/step - loss: 0.3346 - categorical_accuracy: 0.8464\n",
      "Epoch 20/20\n",
      "37337/37337 [==============================] - 0s 13us/step - loss: 0.3346 - categorical_accuracy: 0.8455\n",
      "[[7.67232238e-01 2.12089077e-02 7.79427359e-02 1.90880170e-02\n",
      "  1.14528102e-01]\n",
      " [1.60427807e-03 9.40106952e-01 9.62566845e-03 4.43850267e-02\n",
      "  4.27807487e-03]\n",
      " [4.53103761e-04 0.00000000e+00 9.90937925e-01 6.79655641e-03\n",
      "  1.81241504e-03]\n",
      " [0.00000000e+00 4.58221024e-02 4.04312668e-03 9.25876011e-01\n",
      "  2.42587601e-02]\n",
      " [2.70656780e-01 6.19703390e-02 3.44279661e-02 4.50211864e-02\n",
      "  5.87923729e-01]]\n"
     ]
    }
   ],
   "source": [
    "X = malicious_data_overall.iloc[:,12:] \n",
    "n = X.shape[1]\n",
    "y = malicious_labels\n",
    "X = np.reshape(X.values, (X.shape[0], X.shape[1]))\n",
    "\n",
    "model_all = Sequential()\n",
    "model_all.add(Dense(128, input_dim=n, activation=activation_func_use))\n",
    "model_all.add(Dense(16, activation=activation_func_use))\n",
    "model_all.add(Dense(5, activation=activation_func_use))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, )\n",
    "\n",
    "model_all.compile(loss='categorical_crossentropy', optimizer=optimizer_scheme_use, metrics=['categorical_accuracy'])\n",
    "\n",
    "model_all.fit(X_train, y_train, epochs=20, batch_size=100)\n",
    "\n",
    "t = model_all.predict(X_test)\n",
    "evaluation_matrix = check_multi_classification(t,y_test)\n",
    "print(evaluation_matrix)\n",
    "\n",
    "#evaluate_model(y_predict, y_test)\n",
    "#stats(y, y_train, y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = model_all.predict(X_test)\n",
    "def check_multi_classification(t,y_test):\n",
    "    a = np.argmax(t, axis=1)\n",
    "    b = np.argmax(y_test, axis=1)\n",
    "    length = len(b)\n",
    "    results_table = np.zeros((5,5))\n",
    "    for i in range(length):\n",
    "        if b[i] == 0:\n",
    "            if a[i] == 0:\n",
    "                results_table[0,0]+=1\n",
    "            if a[i] == 1:\n",
    "                results_table[0,1]+=1\n",
    "            if a[i] == 2:\n",
    "                results_table[0,2]+=1\n",
    "            if a[i] == 3:\n",
    "                results_table[0,3]+=1\n",
    "            if a[i] == 4:\n",
    "                results_table[0,4]+=1\n",
    "        if b[i] == 1:\n",
    "            if a[i] == 0:\n",
    "                results_table[1,0]+=1\n",
    "            if a[i] == 1:\n",
    "                results_table[1,1]+=1\n",
    "            if a[i] == 2:\n",
    "                results_table[1,2]+=1\n",
    "            if a[i] == 3:\n",
    "                results_table[1,3]+=1\n",
    "            if a[i] == 4:\n",
    "                results_table[1,4]+=1\n",
    "        if b[i] == 2:\n",
    "            if a[i] == 0:\n",
    "                results_table[2,0]+=1\n",
    "            if a[i] == 1:\n",
    "                results_table[2,1]+=1\n",
    "            if a[i] == 2:\n",
    "                results_table[2,2]+=1\n",
    "            if a[i] == 3:\n",
    "                results_table[2,3]+=1\n",
    "            if a[i] == 4:\n",
    "                results_table[2,4]+=1\n",
    "        if b[i] == 3:\n",
    "            if a[i] == 0:\n",
    "                results_table[3,0]+=1\n",
    "            if a[i] == 1:\n",
    "                results_table[3,1]+=1\n",
    "            if a[i] == 2:\n",
    "                results_table[3,2]+=1\n",
    "            if a[i] == 3:\n",
    "                results_table[3,3]+=1\n",
    "            if a[i] == 4:\n",
    "                results_table[3,4]+=1\n",
    "        if b[i] == 4:\n",
    "            if a[i] == 0:\n",
    "                results_table[4,0]+=1\n",
    "            if a[i] == 1:\n",
    "                results_table[4,1]+=1\n",
    "            if a[i] == 2:\n",
    "                results_table[4,2]+=1\n",
    "            if a[i] == 3:\n",
    "                results_table[4,3]+=1\n",
    "            if a[i] == 4:\n",
    "                results_table[4,4]+=1\n",
    "    MM_per = np.zeros((5,5))\n",
    "    for i in range(5):\n",
    "        MM_per[i,:] = results_table[i,:]/sum(results_table[i,:])\n",
    "    \n",
    "    return MM_per"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "For validation, we randomly choose a transmission session and feed it into the models we trained previously. For each kind of attack, we apply the individual detector and overall detector to make the detection. \n",
    "\n",
    "The mode indicate whether the session that was tested is malicious or normal. \n",
    "\n",
    "The results are follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.feature_each_transmission import location_plausibility, movement_plausibility, quantititative_information, distance_check\n",
    "\n",
    "def get_data(data, mode):\n",
    "    # Randomly pick a transmission session from the dataset. \n",
    "    data_category = data.loc[(data['Label']==mode)]\n",
    "    data_sender_ID = np.unique(np.array(data_category.iloc[:,5]))\n",
    "    n = np.random.randint(len(data_sender_ID))\n",
    "    data_sender = data_category.loc[data_category['tr_ID'] == data_sender_ID[n]]\n",
    "    data_recevier_ID = np.unique(np.array(data_sender.iloc[:,1]))\n",
    "    m = np.random.randint(len(data_recevier_ID))\n",
    "    data_select = data_sender.loc[data_sender['re_ID']==data_recevier_ID[m]]\n",
    "    return data_select\n",
    "\n",
    "\n",
    "def test_real_data(model, data, mode):\n",
    "    \n",
    "    data_vector =np.zeros((1, 7))\n",
    "    data_vector[0][0] = location_plausibility(data)\n",
    "    data_vector[0][1] = movement_plausibility(data)\n",
    "    data_vector[0][2], data_vector[0][3], data_vector[0][4], data_vector[0][5] =quantititative_information(data)\n",
    "    data_vector[0][6] = distance_check(data,800)\n",
    "    \n",
    "    y_predict = np.round(model.predict(data_vector))\n",
    "    \n",
    "    if y_predict==1:\n",
    "        print('The detection system said: The BSM is malicious!')\n",
    "        if mode==1:\n",
    "            print('The detection is correct!')\n",
    "        else:\n",
    "            print('But in fact, the BSM is normal, the detection is incorrect')\n",
    "    if y_predict==0:\n",
    "        print('The detection system said: The BSM is normal!')\n",
    "        if mode==0:\n",
    "            print('The detection is correct!')\n",
    "        else:\n",
    "            print('But in fact, the BSM is malicious, the detection is incorrect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "col_index = [1,2,3,4,6,7,9,10,12,13,15,16]\n",
    "col_names = ['re_time','re_ID','re_x','re_y','tr_time','tr_ID','tr_x','tr_y','tr_vx','tr_vy','RSSI','Label']\n",
    "\n",
    "data = pd.read_csv('dataset/attack1withlabels.csv', usecols = col_index, header = None, names = col_names)\n",
    "data_atk1 = data.dropna(axis = 0, how = 'any')\n",
    "\n",
    "data = pd.read_csv('dataset/attack2withlabels.csv', usecols = col_index, header = None, names = col_names)\n",
    "data_atk2 = data.dropna(axis = 0, how = 'any')\n",
    "data_atk2['Label'] = data_atk2['Label']/2\n",
    "\n",
    "data = pd.read_csv('dataset/attack4withlabels.csv', usecols = col_index, header = None, names = col_names)\n",
    "data_atk4 = data.dropna(axis = 0, how = 'any')\n",
    "data_atk4['Label'] = data_atk4['Label']/4\n",
    "\n",
    "data = pd.read_csv('dataset/attack8withlabels.csv', usecols = col_index, header = None, names = col_names)\n",
    "data_atk8 = data.dropna(axis = 0, how = 'any')\n",
    "data_atk8['Label'] = data_atk8['Label']/8\n",
    "\n",
    "data = pd.read_csv('dataset/attack16withlabels.csv', usecols = col_index, header = None, names = col_names)\n",
    "data_atk16 = data.dropna(axis = 0, how = 'any')\n",
    "data_atk16['Label'] = data_atk16['Label']/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the attack 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>re_time</th>\n",
       "      <th>re_ID</th>\n",
       "      <th>re_x</th>\n",
       "      <th>re_y</th>\n",
       "      <th>tr_time</th>\n",
       "      <th>tr_ID</th>\n",
       "      <th>tr_x</th>\n",
       "      <th>tr_y</th>\n",
       "      <th>tr_vx</th>\n",
       "      <th>tr_vy</th>\n",
       "      <th>RSSI</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>211442</th>\n",
       "      <td>21931</td>\n",
       "      <td>3322</td>\n",
       "      <td>3598.7</td>\n",
       "      <td>5607.5</td>\n",
       "      <td>21931</td>\n",
       "      <td>3469</td>\n",
       "      <td>5560.0</td>\n",
       "      <td>5820.0</td>\n",
       "      <td>-4.8606</td>\n",
       "      <td>31.160</td>\n",
       "      <td>1.650000e-09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211464</th>\n",
       "      <td>21932</td>\n",
       "      <td>3322</td>\n",
       "      <td>3601.6</td>\n",
       "      <td>5572.1</td>\n",
       "      <td>21932</td>\n",
       "      <td>3469</td>\n",
       "      <td>5560.0</td>\n",
       "      <td>5820.0</td>\n",
       "      <td>-4.8574</td>\n",
       "      <td>31.140</td>\n",
       "      <td>1.520000e-08</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211494</th>\n",
       "      <td>21933</td>\n",
       "      <td>3322</td>\n",
       "      <td>3604.6</td>\n",
       "      <td>5536.7</td>\n",
       "      <td>21933</td>\n",
       "      <td>3469</td>\n",
       "      <td>5560.0</td>\n",
       "      <td>5820.0</td>\n",
       "      <td>-4.8649</td>\n",
       "      <td>31.188</td>\n",
       "      <td>1.720000e-09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211521</th>\n",
       "      <td>21934</td>\n",
       "      <td>3322</td>\n",
       "      <td>3607.6</td>\n",
       "      <td>5501.3</td>\n",
       "      <td>21934</td>\n",
       "      <td>3469</td>\n",
       "      <td>5560.0</td>\n",
       "      <td>5820.0</td>\n",
       "      <td>-3.6128</td>\n",
       "      <td>31.311</td>\n",
       "      <td>3.090000e-09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211550</th>\n",
       "      <td>21935</td>\n",
       "      <td>3322</td>\n",
       "      <td>3611.1</td>\n",
       "      <td>5466.0</td>\n",
       "      <td>21935</td>\n",
       "      <td>3469</td>\n",
       "      <td>5560.0</td>\n",
       "      <td>5820.0</td>\n",
       "      <td>-3.6115</td>\n",
       "      <td>31.299</td>\n",
       "      <td>5.930000e-09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211579</th>\n",
       "      <td>21936</td>\n",
       "      <td>3322</td>\n",
       "      <td>3615.5</td>\n",
       "      <td>5430.7</td>\n",
       "      <td>21936</td>\n",
       "      <td>3469</td>\n",
       "      <td>5560.0</td>\n",
       "      <td>5820.0</td>\n",
       "      <td>-3.6221</td>\n",
       "      <td>31.391</td>\n",
       "      <td>2.840000e-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211611</th>\n",
       "      <td>21937</td>\n",
       "      <td>3322</td>\n",
       "      <td>3619.9</td>\n",
       "      <td>5395.5</td>\n",
       "      <td>21937</td>\n",
       "      <td>3469</td>\n",
       "      <td>5560.0</td>\n",
       "      <td>5820.0</td>\n",
       "      <td>-3.6229</td>\n",
       "      <td>31.398</td>\n",
       "      <td>1.800000e-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211642</th>\n",
       "      <td>21938</td>\n",
       "      <td>3322</td>\n",
       "      <td>3624.2</td>\n",
       "      <td>5360.2</td>\n",
       "      <td>21938</td>\n",
       "      <td>3469</td>\n",
       "      <td>5560.0</td>\n",
       "      <td>5820.0</td>\n",
       "      <td>-3.6232</td>\n",
       "      <td>31.401</td>\n",
       "      <td>3.010000e-08</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211671</th>\n",
       "      <td>21939</td>\n",
       "      <td>3322</td>\n",
       "      <td>3628.6</td>\n",
       "      <td>5325.0</td>\n",
       "      <td>21939</td>\n",
       "      <td>3469</td>\n",
       "      <td>5560.0</td>\n",
       "      <td>5820.0</td>\n",
       "      <td>-2.5786</td>\n",
       "      <td>31.478</td>\n",
       "      <td>6.020000e-09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211695</th>\n",
       "      <td>21940</td>\n",
       "      <td>3322</td>\n",
       "      <td>3633.9</td>\n",
       "      <td>5289.9</td>\n",
       "      <td>21940</td>\n",
       "      <td>3469</td>\n",
       "      <td>5560.0</td>\n",
       "      <td>5820.0</td>\n",
       "      <td>-2.5742</td>\n",
       "      <td>31.424</td>\n",
       "      <td>2.310000e-09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211721</th>\n",
       "      <td>21941</td>\n",
       "      <td>3322</td>\n",
       "      <td>3639.4</td>\n",
       "      <td>5255.0</td>\n",
       "      <td>21941</td>\n",
       "      <td>3469</td>\n",
       "      <td>5560.0</td>\n",
       "      <td>5820.0</td>\n",
       "      <td>-2.5708</td>\n",
       "      <td>31.383</td>\n",
       "      <td>1.310000e-09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211752</th>\n",
       "      <td>21942</td>\n",
       "      <td>3322</td>\n",
       "      <td>3644.8</td>\n",
       "      <td>5220.4</td>\n",
       "      <td>21942</td>\n",
       "      <td>3469</td>\n",
       "      <td>5560.0</td>\n",
       "      <td>5820.0</td>\n",
       "      <td>-2.5761</td>\n",
       "      <td>31.447</td>\n",
       "      <td>2.220000e-09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211780</th>\n",
       "      <td>21943</td>\n",
       "      <td>3322</td>\n",
       "      <td>3650.3</td>\n",
       "      <td>5186.0</td>\n",
       "      <td>21943</td>\n",
       "      <td>3469</td>\n",
       "      <td>5560.0</td>\n",
       "      <td>5820.0</td>\n",
       "      <td>-2.5798</td>\n",
       "      <td>31.493</td>\n",
       "      <td>2.100000e-09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        re_time  re_ID    re_x    re_y  tr_time  tr_ID    tr_x    tr_y  \\\n",
       "211442    21931   3322  3598.7  5607.5    21931   3469  5560.0  5820.0   \n",
       "211464    21932   3322  3601.6  5572.1    21932   3469  5560.0  5820.0   \n",
       "211494    21933   3322  3604.6  5536.7    21933   3469  5560.0  5820.0   \n",
       "211521    21934   3322  3607.6  5501.3    21934   3469  5560.0  5820.0   \n",
       "211550    21935   3322  3611.1  5466.0    21935   3469  5560.0  5820.0   \n",
       "211579    21936   3322  3615.5  5430.7    21936   3469  5560.0  5820.0   \n",
       "211611    21937   3322  3619.9  5395.5    21937   3469  5560.0  5820.0   \n",
       "211642    21938   3322  3624.2  5360.2    21938   3469  5560.0  5820.0   \n",
       "211671    21939   3322  3628.6  5325.0    21939   3469  5560.0  5820.0   \n",
       "211695    21940   3322  3633.9  5289.9    21940   3469  5560.0  5820.0   \n",
       "211721    21941   3322  3639.4  5255.0    21941   3469  5560.0  5820.0   \n",
       "211752    21942   3322  3644.8  5220.4    21942   3469  5560.0  5820.0   \n",
       "211780    21943   3322  3650.3  5186.0    21943   3469  5560.0  5820.0   \n",
       "\n",
       "         tr_vx   tr_vy          RSSI  Label  \n",
       "211442 -4.8606  31.160  1.650000e-09      1  \n",
       "211464 -4.8574  31.140  1.520000e-08      1  \n",
       "211494 -4.8649  31.188  1.720000e-09      1  \n",
       "211521 -3.6128  31.311  3.090000e-09      1  \n",
       "211550 -3.6115  31.299  5.930000e-09      1  \n",
       "211579 -3.6221  31.391  2.840000e-07      1  \n",
       "211611 -3.6229  31.398  1.800000e-07      1  \n",
       "211642 -3.6232  31.401  3.010000e-08      1  \n",
       "211671 -2.5786  31.478  6.020000e-09      1  \n",
       "211695 -2.5742  31.424  2.310000e-09      1  \n",
       "211721 -2.5708  31.383  1.310000e-09      1  \n",
       "211752 -2.5761  31.447  2.220000e-09      1  \n",
       "211780 -2.5798  31.493  2.100000e-09      1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode = 1\n",
    "data_select = get_data(data_atk1, mode)\n",
    "data_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********if we use the model 1 to detect the attack1*******\n",
      "The detection system said: The BSM is malicious!\n",
      "The detection is correct!\n",
      "********if we use the model_all to detect the attack1*******\n",
      "The detection system said: The BSM is malicious!\n",
      "The detection is correct!\n"
     ]
    }
   ],
   "source": [
    "print('********if we use the model 1 to detect the attack1*******')\n",
    "test_real_data(model, data_select, 1)\n",
    "print('********if we use the model_all to detect the attack1*******')\n",
    "test_real_data(model_all, data_select, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the attack 2\n",
    "Notice that attack 2 could not be detected by neural network model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>re_time</th>\n",
       "      <th>re_ID</th>\n",
       "      <th>re_x</th>\n",
       "      <th>re_y</th>\n",
       "      <th>tr_time</th>\n",
       "      <th>tr_ID</th>\n",
       "      <th>tr_x</th>\n",
       "      <th>tr_y</th>\n",
       "      <th>tr_vx</th>\n",
       "      <th>tr_vy</th>\n",
       "      <th>RSSI</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>206837</th>\n",
       "      <td>21841</td>\n",
       "      <td>2434</td>\n",
       "      <td>6320.6</td>\n",
       "      <td>5961.6</td>\n",
       "      <td>21841</td>\n",
       "      <td>2209</td>\n",
       "      <td>6562.2</td>\n",
       "      <td>5885.5</td>\n",
       "      <td>0.15119</td>\n",
       "      <td>-1.0653</td>\n",
       "      <td>5.164000e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206842</th>\n",
       "      <td>21842</td>\n",
       "      <td>2434</td>\n",
       "      <td>6322.6</td>\n",
       "      <td>5948.1</td>\n",
       "      <td>21842</td>\n",
       "      <td>2209</td>\n",
       "      <td>6562.4</td>\n",
       "      <td>5883.4</td>\n",
       "      <td>0.38994</td>\n",
       "      <td>-2.9812</td>\n",
       "      <td>1.045400e-08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206849</th>\n",
       "      <td>21843</td>\n",
       "      <td>2434</td>\n",
       "      <td>6324.8</td>\n",
       "      <td>5934.8</td>\n",
       "      <td>21843</td>\n",
       "      <td>2209</td>\n",
       "      <td>6562.8</td>\n",
       "      <td>5879.5</td>\n",
       "      <td>0.52452</td>\n",
       "      <td>-4.5792</td>\n",
       "      <td>1.309200e-08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        re_time  re_ID    re_x    re_y  tr_time  tr_ID    tr_x    tr_y  \\\n",
       "206837    21841   2434  6320.6  5961.6    21841   2209  6562.2  5885.5   \n",
       "206842    21842   2434  6322.6  5948.1    21842   2209  6562.4  5883.4   \n",
       "206849    21843   2434  6324.8  5934.8    21843   2209  6562.8  5879.5   \n",
       "\n",
       "          tr_vx   tr_vy          RSSI  Label  \n",
       "206837  0.15119 -1.0653  5.164000e-09    1.0  \n",
       "206842  0.38994 -2.9812  1.045400e-08    1.0  \n",
       "206849  0.52452 -4.5792  1.309200e-08    1.0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode = 1\n",
    "data_select = get_data(data_atk2, mode)\n",
    "data_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********if we use the model 2 to detect the attack2*******\n",
      "The detection system said: The BSM is normal!\n",
      "But in fact, the BSM is malicious, the detection is incorrect\n",
      "********if we use the model all to detect the attack2*******\n",
      "The detection system said: The BSM is normal!\n",
      "But in fact, the BSM is malicious, the detection is incorrect\n"
     ]
    }
   ],
   "source": [
    "print('********if we use the model 2 to detect the attack2*******')\n",
    "test_real_data(model2, data_select, mode)\n",
    "print('********if we use the model all to detect the attack2*******')\n",
    "test_real_data(model_all, data_select, mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the attack 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>re_time</th>\n",
       "      <th>re_ID</th>\n",
       "      <th>re_x</th>\n",
       "      <th>re_y</th>\n",
       "      <th>tr_time</th>\n",
       "      <th>tr_ID</th>\n",
       "      <th>tr_x</th>\n",
       "      <th>tr_y</th>\n",
       "      <th>tr_vx</th>\n",
       "      <th>tr_vy</th>\n",
       "      <th>RSSI</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>194104</th>\n",
       "      <td>21608</td>\n",
       "      <td>598</td>\n",
       "      <td>3598.0</td>\n",
       "      <td>6005.4</td>\n",
       "      <td>21608</td>\n",
       "      <td>619</td>\n",
       "      <td>980.42</td>\n",
       "      <td>10977.0</td>\n",
       "      <td>-3.428600</td>\n",
       "      <td>-29.787</td>\n",
       "      <td>3.036900e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194114</th>\n",
       "      <td>21609</td>\n",
       "      <td>598</td>\n",
       "      <td>3595.5</td>\n",
       "      <td>5967.3</td>\n",
       "      <td>21609</td>\n",
       "      <td>619</td>\n",
       "      <td>21577.00</td>\n",
       "      <td>20802.0</td>\n",
       "      <td>-3.425600</td>\n",
       "      <td>-29.762</td>\n",
       "      <td>2.622200e-08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194127</th>\n",
       "      <td>21610</td>\n",
       "      <td>598</td>\n",
       "      <td>3593.0</td>\n",
       "      <td>5929.2</td>\n",
       "      <td>21610</td>\n",
       "      <td>619</td>\n",
       "      <td>25562.00</td>\n",
       "      <td>17765.0</td>\n",
       "      <td>-3.426900</td>\n",
       "      <td>-29.773</td>\n",
       "      <td>8.379000e-08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194139</th>\n",
       "      <td>21611</td>\n",
       "      <td>598</td>\n",
       "      <td>3591.5</td>\n",
       "      <td>5891.0</td>\n",
       "      <td>21611</td>\n",
       "      <td>619</td>\n",
       "      <td>17626.00</td>\n",
       "      <td>10081.0</td>\n",
       "      <td>-1.984000</td>\n",
       "      <td>-29.903</td>\n",
       "      <td>1.041800e-07</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194154</th>\n",
       "      <td>21612</td>\n",
       "      <td>598</td>\n",
       "      <td>3591.2</td>\n",
       "      <td>5852.8</td>\n",
       "      <td>21612</td>\n",
       "      <td>619</td>\n",
       "      <td>17333.00</td>\n",
       "      <td>3294.1</td>\n",
       "      <td>-1.978900</td>\n",
       "      <td>-29.827</td>\n",
       "      <td>9.640700e-08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194169</th>\n",
       "      <td>21613</td>\n",
       "      <td>598</td>\n",
       "      <td>3591.2</td>\n",
       "      <td>5814.6</td>\n",
       "      <td>21613</td>\n",
       "      <td>619</td>\n",
       "      <td>2036.60</td>\n",
       "      <td>12536.0</td>\n",
       "      <td>-1.173200</td>\n",
       "      <td>-29.900</td>\n",
       "      <td>3.220700e-08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194184</th>\n",
       "      <td>21614</td>\n",
       "      <td>598</td>\n",
       "      <td>3591.2</td>\n",
       "      <td>5776.4</td>\n",
       "      <td>21614</td>\n",
       "      <td>619</td>\n",
       "      <td>15040.00</td>\n",
       "      <td>7905.5</td>\n",
       "      <td>-1.173500</td>\n",
       "      <td>-29.907</td>\n",
       "      <td>3.643400e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194198</th>\n",
       "      <td>21615</td>\n",
       "      <td>598</td>\n",
       "      <td>3591.5</td>\n",
       "      <td>5738.3</td>\n",
       "      <td>21615</td>\n",
       "      <td>619</td>\n",
       "      <td>1707.10</td>\n",
       "      <td>20244.0</td>\n",
       "      <td>-0.017445</td>\n",
       "      <td>-29.912</td>\n",
       "      <td>9.104600e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194248</th>\n",
       "      <td>21618</td>\n",
       "      <td>598</td>\n",
       "      <td>3597.3</td>\n",
       "      <td>5623.9</td>\n",
       "      <td>21618</td>\n",
       "      <td>619</td>\n",
       "      <td>24134.00</td>\n",
       "      <td>7370.2</td>\n",
       "      <td>-0.017438</td>\n",
       "      <td>-29.900</td>\n",
       "      <td>1.742100e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194263</th>\n",
       "      <td>21619</td>\n",
       "      <td>598</td>\n",
       "      <td>3600.5</td>\n",
       "      <td>5585.8</td>\n",
       "      <td>21619</td>\n",
       "      <td>619</td>\n",
       "      <td>7623.70</td>\n",
       "      <td>21873.0</td>\n",
       "      <td>0.475360</td>\n",
       "      <td>-29.880</td>\n",
       "      <td>6.088900e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194282</th>\n",
       "      <td>21620</td>\n",
       "      <td>598</td>\n",
       "      <td>3603.7</td>\n",
       "      <td>5547.8</td>\n",
       "      <td>21620</td>\n",
       "      <td>619</td>\n",
       "      <td>9425.10</td>\n",
       "      <td>18275.0</td>\n",
       "      <td>1.282600</td>\n",
       "      <td>-29.893</td>\n",
       "      <td>2.205800e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194301</th>\n",
       "      <td>21621</td>\n",
       "      <td>598</td>\n",
       "      <td>3606.9</td>\n",
       "      <td>5509.7</td>\n",
       "      <td>21621</td>\n",
       "      <td>619</td>\n",
       "      <td>19825.00</td>\n",
       "      <td>5557.1</td>\n",
       "      <td>1.283100</td>\n",
       "      <td>-29.907</td>\n",
       "      <td>3.847900e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194323</th>\n",
       "      <td>21622</td>\n",
       "      <td>598</td>\n",
       "      <td>3610.4</td>\n",
       "      <td>5471.7</td>\n",
       "      <td>21622</td>\n",
       "      <td>619</td>\n",
       "      <td>27169.00</td>\n",
       "      <td>18122.0</td>\n",
       "      <td>1.282300</td>\n",
       "      <td>-29.887</td>\n",
       "      <td>3.060600e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194348</th>\n",
       "      <td>21623</td>\n",
       "      <td>598</td>\n",
       "      <td>3615.1</td>\n",
       "      <td>5433.8</td>\n",
       "      <td>21623</td>\n",
       "      <td>619</td>\n",
       "      <td>17400.00</td>\n",
       "      <td>21641.0</td>\n",
       "      <td>2.512200</td>\n",
       "      <td>-29.840</td>\n",
       "      <td>1.227100e-08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194371</th>\n",
       "      <td>21624</td>\n",
       "      <td>598</td>\n",
       "      <td>3619.8</td>\n",
       "      <td>5395.9</td>\n",
       "      <td>21624</td>\n",
       "      <td>619</td>\n",
       "      <td>20861.00</td>\n",
       "      <td>17783.0</td>\n",
       "      <td>2.509100</td>\n",
       "      <td>-29.803</td>\n",
       "      <td>2.407000e-08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194390</th>\n",
       "      <td>21625</td>\n",
       "      <td>598</td>\n",
       "      <td>3624.5</td>\n",
       "      <td>5358.0</td>\n",
       "      <td>21625</td>\n",
       "      <td>619</td>\n",
       "      <td>7912.80</td>\n",
       "      <td>9223.9</td>\n",
       "      <td>2.514000</td>\n",
       "      <td>-29.861</td>\n",
       "      <td>2.822500e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194407</th>\n",
       "      <td>21626</td>\n",
       "      <td>598</td>\n",
       "      <td>3629.2</td>\n",
       "      <td>5320.1</td>\n",
       "      <td>21626</td>\n",
       "      <td>619</td>\n",
       "      <td>7462.70</td>\n",
       "      <td>13617.0</td>\n",
       "      <td>2.510300</td>\n",
       "      <td>-29.817</td>\n",
       "      <td>4.010400e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194427</th>\n",
       "      <td>21627</td>\n",
       "      <td>598</td>\n",
       "      <td>3635.1</td>\n",
       "      <td>5282.5</td>\n",
       "      <td>21627</td>\n",
       "      <td>619</td>\n",
       "      <td>16681.00</td>\n",
       "      <td>21118.0</td>\n",
       "      <td>2.512700</td>\n",
       "      <td>-29.846</td>\n",
       "      <td>2.915300e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194443</th>\n",
       "      <td>21628</td>\n",
       "      <td>598</td>\n",
       "      <td>3640.9</td>\n",
       "      <td>5245.4</td>\n",
       "      <td>21628</td>\n",
       "      <td>619</td>\n",
       "      <td>13927.00</td>\n",
       "      <td>11751.0</td>\n",
       "      <td>3.548900</td>\n",
       "      <td>-29.731</td>\n",
       "      <td>6.895700e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194462</th>\n",
       "      <td>21629</td>\n",
       "      <td>598</td>\n",
       "      <td>3646.6</td>\n",
       "      <td>5209.0</td>\n",
       "      <td>21629</td>\n",
       "      <td>619</td>\n",
       "      <td>5763.30</td>\n",
       "      <td>14494.0</td>\n",
       "      <td>3.681900</td>\n",
       "      <td>-29.719</td>\n",
       "      <td>2.243600e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        re_time  re_ID    re_x    re_y  tr_time  tr_ID      tr_x     tr_y  \\\n",
       "194104    21608    598  3598.0  6005.4    21608    619    980.42  10977.0   \n",
       "194114    21609    598  3595.5  5967.3    21609    619  21577.00  20802.0   \n",
       "194127    21610    598  3593.0  5929.2    21610    619  25562.00  17765.0   \n",
       "194139    21611    598  3591.5  5891.0    21611    619  17626.00  10081.0   \n",
       "194154    21612    598  3591.2  5852.8    21612    619  17333.00   3294.1   \n",
       "194169    21613    598  3591.2  5814.6    21613    619   2036.60  12536.0   \n",
       "194184    21614    598  3591.2  5776.4    21614    619  15040.00   7905.5   \n",
       "194198    21615    598  3591.5  5738.3    21615    619   1707.10  20244.0   \n",
       "194248    21618    598  3597.3  5623.9    21618    619  24134.00   7370.2   \n",
       "194263    21619    598  3600.5  5585.8    21619    619   7623.70  21873.0   \n",
       "194282    21620    598  3603.7  5547.8    21620    619   9425.10  18275.0   \n",
       "194301    21621    598  3606.9  5509.7    21621    619  19825.00   5557.1   \n",
       "194323    21622    598  3610.4  5471.7    21622    619  27169.00  18122.0   \n",
       "194348    21623    598  3615.1  5433.8    21623    619  17400.00  21641.0   \n",
       "194371    21624    598  3619.8  5395.9    21624    619  20861.00  17783.0   \n",
       "194390    21625    598  3624.5  5358.0    21625    619   7912.80   9223.9   \n",
       "194407    21626    598  3629.2  5320.1    21626    619   7462.70  13617.0   \n",
       "194427    21627    598  3635.1  5282.5    21627    619  16681.00  21118.0   \n",
       "194443    21628    598  3640.9  5245.4    21628    619  13927.00  11751.0   \n",
       "194462    21629    598  3646.6  5209.0    21629    619   5763.30  14494.0   \n",
       "\n",
       "           tr_vx   tr_vy          RSSI  Label  \n",
       "194104 -3.428600 -29.787  3.036900e-09    1.0  \n",
       "194114 -3.425600 -29.762  2.622200e-08    1.0  \n",
       "194127 -3.426900 -29.773  8.379000e-08    1.0  \n",
       "194139 -1.984000 -29.903  1.041800e-07    1.0  \n",
       "194154 -1.978900 -29.827  9.640700e-08    1.0  \n",
       "194169 -1.173200 -29.900  3.220700e-08    1.0  \n",
       "194184 -1.173500 -29.907  3.643400e-09    1.0  \n",
       "194198 -0.017445 -29.912  9.104600e-09    1.0  \n",
       "194248 -0.017438 -29.900  1.742100e-09    1.0  \n",
       "194263  0.475360 -29.880  6.088900e-09    1.0  \n",
       "194282  1.282600 -29.893  2.205800e-09    1.0  \n",
       "194301  1.283100 -29.907  3.847900e-09    1.0  \n",
       "194323  1.282300 -29.887  3.060600e-09    1.0  \n",
       "194348  2.512200 -29.840  1.227100e-08    1.0  \n",
       "194371  2.509100 -29.803  2.407000e-08    1.0  \n",
       "194390  2.514000 -29.861  2.822500e-09    1.0  \n",
       "194407  2.510300 -29.817  4.010400e-09    1.0  \n",
       "194427  2.512700 -29.846  2.915300e-09    1.0  \n",
       "194443  3.548900 -29.731  6.895700e-09    1.0  \n",
       "194462  3.681900 -29.719  2.243600e-09    1.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode = 1\n",
    "data_select = get_data(data_atk4, mode)\n",
    "data_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********if we use the model 4 to detect the attack4*******\n",
      "The detection system said: The BSM is malicious!\n",
      "The detection is correct!\n",
      "********if we use the model_all to detect the attack4*******\n",
      "The detection system said: The BSM is malicious!\n",
      "The detection is correct!\n"
     ]
    }
   ],
   "source": [
    "print('********if we use the model 4 to detect the attack4*******')\n",
    "test_real_data(model4, data_select, mode)\n",
    "print('********if we use the model_all to detect the attack4*******')\n",
    "test_real_data(model_all, data_select, mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the attack 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>re_time</th>\n",
       "      <th>re_ID</th>\n",
       "      <th>re_x</th>\n",
       "      <th>re_y</th>\n",
       "      <th>tr_time</th>\n",
       "      <th>tr_ID</th>\n",
       "      <th>tr_x</th>\n",
       "      <th>tr_y</th>\n",
       "      <th>tr_vx</th>\n",
       "      <th>tr_vy</th>\n",
       "      <th>RSSI</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>415525</th>\n",
       "      <td>21850</td>\n",
       "      <td>2758</td>\n",
       "      <td>3642.1</td>\n",
       "      <td>5182.2</td>\n",
       "      <td>21850</td>\n",
       "      <td>2587</td>\n",
       "      <td>3748.3</td>\n",
       "      <td>5269.9</td>\n",
       "      <td>-2.255100</td>\n",
       "      <td>27.529</td>\n",
       "      <td>1.848300e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415633</th>\n",
       "      <td>21853</td>\n",
       "      <td>2758</td>\n",
       "      <td>3626.8</td>\n",
       "      <td>5280.3</td>\n",
       "      <td>21853</td>\n",
       "      <td>2587</td>\n",
       "      <td>3701.8</td>\n",
       "      <td>5860.4</td>\n",
       "      <td>-2.253100</td>\n",
       "      <td>27.505</td>\n",
       "      <td>1.907200e-08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415658</th>\n",
       "      <td>21854</td>\n",
       "      <td>2758</td>\n",
       "      <td>3621.7</td>\n",
       "      <td>5313.0</td>\n",
       "      <td>21854</td>\n",
       "      <td>2587</td>\n",
       "      <td>3397.2</td>\n",
       "      <td>5593.0</td>\n",
       "      <td>-2.256000</td>\n",
       "      <td>27.539</td>\n",
       "      <td>3.326000e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415701</th>\n",
       "      <td>21855</td>\n",
       "      <td>2758</td>\n",
       "      <td>3617.7</td>\n",
       "      <td>5345.8</td>\n",
       "      <td>21855</td>\n",
       "      <td>2587</td>\n",
       "      <td>3669.1</td>\n",
       "      <td>5660.0</td>\n",
       "      <td>-1.156000</td>\n",
       "      <td>27.687</td>\n",
       "      <td>1.445600e-08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415740</th>\n",
       "      <td>21856</td>\n",
       "      <td>2758</td>\n",
       "      <td>3613.9</td>\n",
       "      <td>5378.7</td>\n",
       "      <td>21856</td>\n",
       "      <td>2587</td>\n",
       "      <td>3471.5</td>\n",
       "      <td>5691.5</td>\n",
       "      <td>-1.148600</td>\n",
       "      <td>27.510</td>\n",
       "      <td>6.393700e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415862</th>\n",
       "      <td>21859</td>\n",
       "      <td>2758</td>\n",
       "      <td>3602.6</td>\n",
       "      <td>5477.4</td>\n",
       "      <td>21859</td>\n",
       "      <td>2587</td>\n",
       "      <td>3850.8</td>\n",
       "      <td>5916.5</td>\n",
       "      <td>-0.026859</td>\n",
       "      <td>27.684</td>\n",
       "      <td>1.082800e-08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415900</th>\n",
       "      <td>21860</td>\n",
       "      <td>2758</td>\n",
       "      <td>3599.8</td>\n",
       "      <td>5510.3</td>\n",
       "      <td>21860</td>\n",
       "      <td>2587</td>\n",
       "      <td>3641.1</td>\n",
       "      <td>5656.7</td>\n",
       "      <td>-0.026870</td>\n",
       "      <td>27.695</td>\n",
       "      <td>4.210300e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415937</th>\n",
       "      <td>21861</td>\n",
       "      <td>2758</td>\n",
       "      <td>3597.1</td>\n",
       "      <td>5543.3</td>\n",
       "      <td>21861</td>\n",
       "      <td>2587</td>\n",
       "      <td>3554.7</td>\n",
       "      <td>5599.7</td>\n",
       "      <td>-0.026805</td>\n",
       "      <td>27.627</td>\n",
       "      <td>3.148300e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415972</th>\n",
       "      <td>21862</td>\n",
       "      <td>2758</td>\n",
       "      <td>3594.4</td>\n",
       "      <td>5576.3</td>\n",
       "      <td>21862</td>\n",
       "      <td>2587</td>\n",
       "      <td>3868.8</td>\n",
       "      <td>5730.7</td>\n",
       "      <td>-0.026754</td>\n",
       "      <td>27.575</td>\n",
       "      <td>1.562700e-08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416002</th>\n",
       "      <td>21863</td>\n",
       "      <td>2758</td>\n",
       "      <td>3591.7</td>\n",
       "      <td>5609.3</td>\n",
       "      <td>21863</td>\n",
       "      <td>2587</td>\n",
       "      <td>3882.5</td>\n",
       "      <td>5594.4</td>\n",
       "      <td>-0.026762</td>\n",
       "      <td>27.583</td>\n",
       "      <td>5.271500e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416031</th>\n",
       "      <td>21864</td>\n",
       "      <td>2758</td>\n",
       "      <td>3589.0</td>\n",
       "      <td>5642.3</td>\n",
       "      <td>21864</td>\n",
       "      <td>2587</td>\n",
       "      <td>3322.3</td>\n",
       "      <td>5805.9</td>\n",
       "      <td>1.175500</td>\n",
       "      <td>27.610</td>\n",
       "      <td>8.806800e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416061</th>\n",
       "      <td>21865</td>\n",
       "      <td>2758</td>\n",
       "      <td>3587.6</td>\n",
       "      <td>5675.4</td>\n",
       "      <td>21865</td>\n",
       "      <td>2587</td>\n",
       "      <td>3871.9</td>\n",
       "      <td>5928.7</td>\n",
       "      <td>1.175400</td>\n",
       "      <td>27.606</td>\n",
       "      <td>2.143700e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416089</th>\n",
       "      <td>21866</td>\n",
       "      <td>2758</td>\n",
       "      <td>3586.2</td>\n",
       "      <td>5708.4</td>\n",
       "      <td>21866</td>\n",
       "      <td>2587</td>\n",
       "      <td>3721.7</td>\n",
       "      <td>5929.6</td>\n",
       "      <td>2.061100</td>\n",
       "      <td>27.680</td>\n",
       "      <td>1.412000e-08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416116</th>\n",
       "      <td>21867</td>\n",
       "      <td>2758</td>\n",
       "      <td>3584.8</td>\n",
       "      <td>5741.5</td>\n",
       "      <td>21867</td>\n",
       "      <td>2587</td>\n",
       "      <td>3564.2</td>\n",
       "      <td>5813.5</td>\n",
       "      <td>2.050000</td>\n",
       "      <td>27.532</td>\n",
       "      <td>9.340800e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416149</th>\n",
       "      <td>21868</td>\n",
       "      <td>2758</td>\n",
       "      <td>3584.4</td>\n",
       "      <td>5774.6</td>\n",
       "      <td>21868</td>\n",
       "      <td>2587</td>\n",
       "      <td>3644.6</td>\n",
       "      <td>5854.1</td>\n",
       "      <td>2.587900</td>\n",
       "      <td>27.389</td>\n",
       "      <td>1.894700e-08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416175</th>\n",
       "      <td>21869</td>\n",
       "      <td>2758</td>\n",
       "      <td>3584.4</td>\n",
       "      <td>5807.7</td>\n",
       "      <td>21869</td>\n",
       "      <td>2587</td>\n",
       "      <td>3703.6</td>\n",
       "      <td>6235.4</td>\n",
       "      <td>2.894400</td>\n",
       "      <td>27.409</td>\n",
       "      <td>3.036200e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416197</th>\n",
       "      <td>21870</td>\n",
       "      <td>2758</td>\n",
       "      <td>3584.4</td>\n",
       "      <td>5840.8</td>\n",
       "      <td>21870</td>\n",
       "      <td>2587</td>\n",
       "      <td>3488.9</td>\n",
       "      <td>6093.5</td>\n",
       "      <td>2.908300</td>\n",
       "      <td>27.541</td>\n",
       "      <td>1.366100e-08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        re_time  re_ID    re_x    re_y  tr_time  tr_ID    tr_x    tr_y  \\\n",
       "415525    21850   2758  3642.1  5182.2    21850   2587  3748.3  5269.9   \n",
       "415633    21853   2758  3626.8  5280.3    21853   2587  3701.8  5860.4   \n",
       "415658    21854   2758  3621.7  5313.0    21854   2587  3397.2  5593.0   \n",
       "415701    21855   2758  3617.7  5345.8    21855   2587  3669.1  5660.0   \n",
       "415740    21856   2758  3613.9  5378.7    21856   2587  3471.5  5691.5   \n",
       "415862    21859   2758  3602.6  5477.4    21859   2587  3850.8  5916.5   \n",
       "415900    21860   2758  3599.8  5510.3    21860   2587  3641.1  5656.7   \n",
       "415937    21861   2758  3597.1  5543.3    21861   2587  3554.7  5599.7   \n",
       "415972    21862   2758  3594.4  5576.3    21862   2587  3868.8  5730.7   \n",
       "416002    21863   2758  3591.7  5609.3    21863   2587  3882.5  5594.4   \n",
       "416031    21864   2758  3589.0  5642.3    21864   2587  3322.3  5805.9   \n",
       "416061    21865   2758  3587.6  5675.4    21865   2587  3871.9  5928.7   \n",
       "416089    21866   2758  3586.2  5708.4    21866   2587  3721.7  5929.6   \n",
       "416116    21867   2758  3584.8  5741.5    21867   2587  3564.2  5813.5   \n",
       "416149    21868   2758  3584.4  5774.6    21868   2587  3644.6  5854.1   \n",
       "416175    21869   2758  3584.4  5807.7    21869   2587  3703.6  6235.4   \n",
       "416197    21870   2758  3584.4  5840.8    21870   2587  3488.9  6093.5   \n",
       "\n",
       "           tr_vx   tr_vy          RSSI  Label  \n",
       "415525 -2.255100  27.529  1.848300e-09    1.0  \n",
       "415633 -2.253100  27.505  1.907200e-08    1.0  \n",
       "415658 -2.256000  27.539  3.326000e-09    1.0  \n",
       "415701 -1.156000  27.687  1.445600e-08    1.0  \n",
       "415740 -1.148600  27.510  6.393700e-09    1.0  \n",
       "415862 -0.026859  27.684  1.082800e-08    1.0  \n",
       "415900 -0.026870  27.695  4.210300e-09    1.0  \n",
       "415937 -0.026805  27.627  3.148300e-09    1.0  \n",
       "415972 -0.026754  27.575  1.562700e-08    1.0  \n",
       "416002 -0.026762  27.583  5.271500e-09    1.0  \n",
       "416031  1.175500  27.610  8.806800e-09    1.0  \n",
       "416061  1.175400  27.606  2.143700e-09    1.0  \n",
       "416089  2.061100  27.680  1.412000e-08    1.0  \n",
       "416116  2.050000  27.532  9.340800e-09    1.0  \n",
       "416149  2.587900  27.389  1.894700e-08    1.0  \n",
       "416175  2.894400  27.409  3.036200e-09    1.0  \n",
       "416197  2.908300  27.541  1.366100e-08    1.0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode = 1\n",
    "data_select = get_data(data_atk8, mode)\n",
    "data_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********if we use the model 8 to detect the attack8*******\n",
      "The detection system said: The BSM is malicious!\n",
      "The detection is correct!\n",
      "********if we use the model all to detect the attack8*******\n",
      "The detection system said: The BSM is malicious!\n",
      "The detection is correct!\n"
     ]
    }
   ],
   "source": [
    "print('********if we use the model 8 to detect the attack8*******')\n",
    "test_real_data(model8, data_select, 1)\n",
    "print('********if we use the model all to detect the attack8*******')\n",
    "test_real_data(model_all, data_select, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the attack 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>re_time</th>\n",
       "      <th>re_ID</th>\n",
       "      <th>re_x</th>\n",
       "      <th>re_y</th>\n",
       "      <th>tr_time</th>\n",
       "      <th>tr_ID</th>\n",
       "      <th>tr_x</th>\n",
       "      <th>tr_y</th>\n",
       "      <th>tr_vx</th>\n",
       "      <th>tr_vy</th>\n",
       "      <th>RSSI</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47063</th>\n",
       "      <td>21659</td>\n",
       "      <td>988</td>\n",
       "      <td>3603.5</td>\n",
       "      <td>5505.5</td>\n",
       "      <td>21659</td>\n",
       "      <td>1075</td>\n",
       "      <td>3600.1</td>\n",
       "      <td>6024.0</td>\n",
       "      <td>-2.434800</td>\n",
       "      <td>-36.679</td>\n",
       "      <td>4.316100e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47086</th>\n",
       "      <td>21660</td>\n",
       "      <td>988</td>\n",
       "      <td>3601.0</td>\n",
       "      <td>5535.3</td>\n",
       "      <td>21660</td>\n",
       "      <td>1075</td>\n",
       "      <td>3600.1</td>\n",
       "      <td>6024.0</td>\n",
       "      <td>-1.425000</td>\n",
       "      <td>-36.476</td>\n",
       "      <td>1.524200e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47107</th>\n",
       "      <td>21661</td>\n",
       "      <td>988</td>\n",
       "      <td>3598.6</td>\n",
       "      <td>5565.1</td>\n",
       "      <td>21661</td>\n",
       "      <td>1075</td>\n",
       "      <td>3600.1</td>\n",
       "      <td>6024.0</td>\n",
       "      <td>-0.021096</td>\n",
       "      <td>-36.245</td>\n",
       "      <td>1.488100e-08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47129</th>\n",
       "      <td>21662</td>\n",
       "      <td>988</td>\n",
       "      <td>3596.1</td>\n",
       "      <td>5594.9</td>\n",
       "      <td>21662</td>\n",
       "      <td>1075</td>\n",
       "      <td>3600.1</td>\n",
       "      <td>6024.0</td>\n",
       "      <td>-0.020984</td>\n",
       "      <td>-36.053</td>\n",
       "      <td>5.180500e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47152</th>\n",
       "      <td>21663</td>\n",
       "      <td>988</td>\n",
       "      <td>3593.7</td>\n",
       "      <td>5624.8</td>\n",
       "      <td>21663</td>\n",
       "      <td>1075</td>\n",
       "      <td>3600.1</td>\n",
       "      <td>6024.0</td>\n",
       "      <td>-0.020868</td>\n",
       "      <td>-35.853</td>\n",
       "      <td>1.431600e-08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47175</th>\n",
       "      <td>21664</td>\n",
       "      <td>988</td>\n",
       "      <td>3591.7</td>\n",
       "      <td>5654.6</td>\n",
       "      <td>21664</td>\n",
       "      <td>1075</td>\n",
       "      <td>3600.1</td>\n",
       "      <td>6024.0</td>\n",
       "      <td>-0.020762</td>\n",
       "      <td>-35.671</td>\n",
       "      <td>4.542600e-08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47200</th>\n",
       "      <td>21665</td>\n",
       "      <td>988</td>\n",
       "      <td>3590.5</td>\n",
       "      <td>5684.5</td>\n",
       "      <td>21665</td>\n",
       "      <td>1075</td>\n",
       "      <td>3600.1</td>\n",
       "      <td>6024.0</td>\n",
       "      <td>1.524800</td>\n",
       "      <td>-35.476</td>\n",
       "      <td>9.050800e-08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47222</th>\n",
       "      <td>21666</td>\n",
       "      <td>988</td>\n",
       "      <td>3589.2</td>\n",
       "      <td>5714.3</td>\n",
       "      <td>21666</td>\n",
       "      <td>1075</td>\n",
       "      <td>3600.1</td>\n",
       "      <td>6024.0</td>\n",
       "      <td>1.516900</td>\n",
       "      <td>-35.292</td>\n",
       "      <td>2.405700e-07</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47248</th>\n",
       "      <td>21667</td>\n",
       "      <td>988</td>\n",
       "      <td>3588.0</td>\n",
       "      <td>5744.2</td>\n",
       "      <td>21667</td>\n",
       "      <td>1075</td>\n",
       "      <td>3600.1</td>\n",
       "      <td>6024.0</td>\n",
       "      <td>1.513500</td>\n",
       "      <td>-35.212</td>\n",
       "      <td>5.307400e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47293</th>\n",
       "      <td>21669</td>\n",
       "      <td>988</td>\n",
       "      <td>3587.7</td>\n",
       "      <td>5804.0</td>\n",
       "      <td>21669</td>\n",
       "      <td>1075</td>\n",
       "      <td>3600.1</td>\n",
       "      <td>6024.0</td>\n",
       "      <td>2.939300</td>\n",
       "      <td>-34.917</td>\n",
       "      <td>5.352300e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47321</th>\n",
       "      <td>21670</td>\n",
       "      <td>988</td>\n",
       "      <td>3587.7</td>\n",
       "      <td>5833.9</td>\n",
       "      <td>21670</td>\n",
       "      <td>1075</td>\n",
       "      <td>3600.1</td>\n",
       "      <td>6024.0</td>\n",
       "      <td>2.936900</td>\n",
       "      <td>-34.889</td>\n",
       "      <td>3.354800e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47341</th>\n",
       "      <td>21671</td>\n",
       "      <td>988</td>\n",
       "      <td>3587.6</td>\n",
       "      <td>5863.8</td>\n",
       "      <td>21671</td>\n",
       "      <td>1075</td>\n",
       "      <td>3600.1</td>\n",
       "      <td>6024.0</td>\n",
       "      <td>2.927200</td>\n",
       "      <td>-34.774</td>\n",
       "      <td>5.368900e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47438</th>\n",
       "      <td>21676</td>\n",
       "      <td>988</td>\n",
       "      <td>3595.9</td>\n",
       "      <td>6013.0</td>\n",
       "      <td>21676</td>\n",
       "      <td>1075</td>\n",
       "      <td>3600.1</td>\n",
       "      <td>6024.0</td>\n",
       "      <td>4.251200</td>\n",
       "      <td>-34.332</td>\n",
       "      <td>1.283400e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       re_time  re_ID    re_x    re_y  tr_time  tr_ID    tr_x    tr_y  \\\n",
       "47063    21659    988  3603.5  5505.5    21659   1075  3600.1  6024.0   \n",
       "47086    21660    988  3601.0  5535.3    21660   1075  3600.1  6024.0   \n",
       "47107    21661    988  3598.6  5565.1    21661   1075  3600.1  6024.0   \n",
       "47129    21662    988  3596.1  5594.9    21662   1075  3600.1  6024.0   \n",
       "47152    21663    988  3593.7  5624.8    21663   1075  3600.1  6024.0   \n",
       "47175    21664    988  3591.7  5654.6    21664   1075  3600.1  6024.0   \n",
       "47200    21665    988  3590.5  5684.5    21665   1075  3600.1  6024.0   \n",
       "47222    21666    988  3589.2  5714.3    21666   1075  3600.1  6024.0   \n",
       "47248    21667    988  3588.0  5744.2    21667   1075  3600.1  6024.0   \n",
       "47293    21669    988  3587.7  5804.0    21669   1075  3600.1  6024.0   \n",
       "47321    21670    988  3587.7  5833.9    21670   1075  3600.1  6024.0   \n",
       "47341    21671    988  3587.6  5863.8    21671   1075  3600.1  6024.0   \n",
       "47438    21676    988  3595.9  6013.0    21676   1075  3600.1  6024.0   \n",
       "\n",
       "          tr_vx   tr_vy          RSSI  Label  \n",
       "47063 -2.434800 -36.679  4.316100e-09    1.0  \n",
       "47086 -1.425000 -36.476  1.524200e-09    1.0  \n",
       "47107 -0.021096 -36.245  1.488100e-08    1.0  \n",
       "47129 -0.020984 -36.053  5.180500e-09    1.0  \n",
       "47152 -0.020868 -35.853  1.431600e-08    1.0  \n",
       "47175 -0.020762 -35.671  4.542600e-08    1.0  \n",
       "47200  1.524800 -35.476  9.050800e-08    1.0  \n",
       "47222  1.516900 -35.292  2.405700e-07    1.0  \n",
       "47248  1.513500 -35.212  5.307400e-09    1.0  \n",
       "47293  2.939300 -34.917  5.352300e-09    1.0  \n",
       "47321  2.936900 -34.889  3.354800e-09    1.0  \n",
       "47341  2.927200 -34.774  5.368900e-09    1.0  \n",
       "47438  4.251200 -34.332  1.283400e-09    1.0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode = 1\n",
    "data_select = get_data(data_atk16, mode)\n",
    "data_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********if we use the model 16 to detect the attack16*******\n",
      "The detection system said: The BSM is malicious!\n",
      "The detection is correct!\n",
      "********if we use the model all to detect the attack16*******\n",
      "The detection system said: The BSM is malicious!\n",
      "The detection is correct!\n"
     ]
    }
   ],
   "source": [
    "print('********if we use the model 16 to detect the attack16*******')\n",
    "test_real_data(model16, data_select, mode)\n",
    "print('********if we use the model all to detect the attack16*******')\n",
    "test_real_data(model_all, data_select, mode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
