{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net\n",
    "In this file, we test the neural net for all kind of attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense,Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_attack1 = pd.read_csv(\"dataset/attack1with7FeatureVector.csv\")\n",
    "data_attack1 = data_attack1.dropna(axis=0, how=\"any\")#remove invalid data\n",
    "sum(np.array(data_attack1['Label']))/data_attack1['Label'].shape[0]/1\n",
    "\n",
    "data_attack1.shape\n",
    "\n",
    "X = data_attack1.iloc[:,12:] #In our case, we use the feature of 1-6, the feature of distance rejected\n",
    "n = X.shape[1]\n",
    "y = data_attack1.iloc[:,11]\n",
    "X = np.reshape(X.values, (X.shape[0], X.shape[1]))\n",
    "y =np.reshape(y.values, (y.shape[0],  1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_func = ['sigmoid','softmax', 'elu', 'selu', 'softplus', 'softsign', \n",
    "                   'relu', 'tanh', 'hard_sigmoid', 'linear']\n",
    "loss_func = ['binary_crossentropy', 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error',\n",
    "            'mean_squared_logarithmic_error', 'squared_hinge', 'hinge', 'logcosh', 'kullback_leibler_divergence', \n",
    "            'poisson', 'cosine_proximity']\n",
    "optimizer_scheme = ['Adagrad','SGD', 'RMSprop', 'Adadelta', 'Adam', 'Adamax', 'Nadam']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********For the activation function sigmoid ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 3s 115us/step - loss: 0.2750 - binary_accuracy: 0.9193\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 1s 22us/step - loss: 0.1346 - binary_accuracy: 0.9622\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 20us/step - loss: 0.1006 - binary_accuracy: 0.9745\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 18us/step - loss: 0.0860 - binary_accuracy: 0.9827\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 19us/step - loss: 0.0775 - binary_accuracy: 0.9837\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 17us/step - loss: 0.0721 - binary_accuracy: 0.9841\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 17us/step - loss: 0.0681 - binary_accuracy: 0.9848\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 19us/step - loss: 0.0658 - binary_accuracy: 0.9850\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 18us/step - loss: 0.0634 - binary_accuracy: 0.9855\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 17us/step - loss: 0.0619 - binary_accuracy: 0.9861\n",
      "6118/6118 [==============================] - 1s 241us/step\n",
      "For attack1: The loss is 0.056002680070992975 , the accuracy is 0.9877410913729597\n",
      "********For the activation function softmax ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 5s 186us/step - loss: 10.9167 - binary_accuracy: 0.3152\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 1s 34us/step - loss: 10.9167 - binary_accuracy: 0.3152\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 1s 36us/step - loss: 10.9167 - binary_accuracy: 0.3152\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 1s 37us/step - loss: 10.9167 - binary_accuracy: 0.3152\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 1s 37us/step - loss: 10.9167 - binary_accuracy: 0.3152\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 1s 36us/step - loss: 10.9167 - binary_accuracy: 0.3152\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 1s 36us/step - loss: 10.9167 - binary_accuracy: 0.3152\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 1s 37us/step - loss: 10.9167 - binary_accuracy: 0.3152\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 1s 37us/step - loss: 10.9167 - binary_accuracy: 0.3152\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 1s 37us/step - loss: 10.9167 - binary_accuracy: 0.3152\n",
      "6118/6118 [==============================] - 2s 261us/step\n",
      "For attack1: The loss is 10.988727391253269 , the accuracy is 0.3107224584561074\n",
      "********For the activation function elu ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 4s 159us/step - loss: 1.2320 - binary_accuracy: 0.4437\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 17us/step - loss: 0.4453 - binary_accuracy: 0.3821\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 17us/step - loss: 0.4285 - binary_accuracy: 0.3846\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 17us/step - loss: 0.4065 - binary_accuracy: 0.3855\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 17us/step - loss: 0.4078 - binary_accuracy: 0.3894\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 17us/step - loss: 0.3801 - binary_accuracy: 0.3987\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 1s 29us/step - loss: 0.4098 - binary_accuracy: 0.4009\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 1s 32us/step - loss: 0.3796 - binary_accuracy: 0.4050\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 1s 34us/step - loss: 0.6679 - binary_accuracy: 0.4933\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 1s 32us/step - loss: 0.7571 - binary_accuracy: 0.4914\n",
      "6118/6118 [==============================] - 2s 257us/step\n",
      "For attack1: The loss is 0.7962449373237134 , the accuracy is 0.4727034978946076\n",
      "********For the activation function selu ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 5s 201us/step - loss: 3.5334 - binary_accuracy: 0.0898\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 1s 34us/step - loss: 1.0141 - binary_accuracy: 0.1665\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 1s 34us/step - loss: 1.0100 - binary_accuracy: 0.1626\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 1s 34us/step - loss: 2.6959 - binary_accuracy: 0.1247\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 1s 34us/step - loss: 1.0065 - binary_accuracy: 0.0985\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 1s 35us/step - loss: 0.9432 - binary_accuracy: 0.1268\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 1s 35us/step - loss: 4.0609 - binary_accuracy: 0.1191\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 1s 33us/step - loss: 4.4552 - binary_accuracy: 0.1223\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 1s 34us/step - loss: 4.4546 - binary_accuracy: 0.1180\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 1s 33us/step - loss: 4.4533 - binary_accuracy: 0.1205\n",
      "6118/6118 [==============================] - 2s 261us/step\n",
      "For attack1: The loss is 4.448660283121419 , the accuracy is 0.11212814656512805\n",
      "********For the activation function softplus ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 5s 201us/step - loss: 10.9597 - binary_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 1s 33us/step - loss: 10.9597 - binary_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 1s 38us/step - loss: 10.9597 - binary_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 1s 38us/step - loss: 10.9597 - binary_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 1s 39us/step - loss: 10.9597 - binary_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 1s 36us/step - loss: 10.9597 - binary_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 1s 36us/step - loss: 10.9597 - binary_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 1s 38us/step - loss: 10.9597 - binary_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 1s 39us/step - loss: 10.9597 - binary_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 1s 40us/step - loss: 10.9597 - binary_accuracy: 0.0000e+00\n",
      "6118/6118 [==============================] - 2s 277us/step\n",
      "For attack1: The loss is 10.816743533336169 , the accuracy is 0.0\n",
      "********For the activation function softsign ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 5s 207us/step - loss: 0.3296 - binary_accuracy: 0.9464\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 1s 38us/step - loss: 0.2404 - binary_accuracy: 0.9598\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 1s 31us/step - loss: 0.2240 - binary_accuracy: 0.6665\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 1s 30us/step - loss: 0.2108 - binary_accuracy: 0.5054\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 1s 30us/step - loss: 0.2066 - binary_accuracy: 0.4445\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 1s 30us/step - loss: 0.1981 - binary_accuracy: 0.4301\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 1s 31us/step - loss: 0.2037 - binary_accuracy: 0.4068\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 1s 37us/step - loss: 0.1972 - binary_accuracy: 0.3738\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 1s 32us/step - loss: 0.1900 - binary_accuracy: 0.3819\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 1s 30us/step - loss: 0.1917 - binary_accuracy: 0.3862\n",
      "6118/6118 [==============================] - 2s 274us/step\n",
      "For attack1: The loss is 0.15436317536635272 , the accuracy is 0.3906505394796407\n",
      "********For the activation function relu ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 5s 205us/step - loss: 3.2690 - binary_accuracy: 0.6973\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 1s 33us/step - loss: 1.8795 - binary_accuracy: 0.6281\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 1s 32us/step - loss: 0.9251 - binary_accuracy: 0.6569\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 1s 32us/step - loss: 0.2848 - binary_accuracy: 0.7091\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 1s 32us/step - loss: 0.4810 - binary_accuracy: 0.7051\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 1s 31us/step - loss: 3.7229 - binary_accuracy: 0.7139\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 1s 31us/step - loss: 3.4483 - binary_accuracy: 0.7115\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 1s 33us/step - loss: 1.0315 - binary_accuracy: 0.6504\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 1s 33us/step - loss: 0.5735 - binary_accuracy: 0.6707\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 1s 32us/step - loss: 0.3128 - binary_accuracy: 0.6949\n",
      "6118/6118 [==============================] - 2s 277us/step\n",
      "For attack1: The loss is 0.11324128096459854 , the accuracy is 0.7204968942150878\n",
      "********For the activation function tanh ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 5s 202us/step - loss: 4.6348 - binary_accuracy: 0.1870\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 1s 30us/step - loss: 4.5907 - binary_accuracy: 0.1526\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 1s 32us/step - loss: 4.5653 - binary_accuracy: 0.1848\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 1s 32us/step - loss: 4.5613 - binary_accuracy: 0.1821\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 1s 31us/step - loss: 4.5593 - binary_accuracy: 0.1859\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 1s 30us/step - loss: 4.5595 - binary_accuracy: 0.1853\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 1s 31us/step - loss: 4.5578 - binary_accuracy: 0.1864\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 1s 32us/step - loss: 4.5559 - binary_accuracy: 0.1919\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 1s 33us/step - loss: 4.5493 - binary_accuracy: 0.2035\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 1s 31us/step - loss: 4.5479 - binary_accuracy: 0.2036\n",
      "6118/6118 [==============================] - 2s 288us/step\n",
      "For attack1: The loss is 4.413847927487571 , the accuracy is 0.2054593006587956\n",
      "********For the activation function hard_sigmoid ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 5s 209us/step - loss: 0.2463 - binary_accuracy: 0.9128\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 1s 34us/step - loss: 0.1249 - binary_accuracy: 0.9498\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 1s 36us/step - loss: 0.1127 - binary_accuracy: 0.9623\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 1s 36us/step - loss: 0.1061 - binary_accuracy: 0.9657\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 1s 34us/step - loss: 0.1001 - binary_accuracy: 0.9678\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 1s 34us/step - loss: 0.0969 - binary_accuracy: 0.9692\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 1s 35us/step - loss: 0.0930 - binary_accuracy: 0.9813\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 1s 34us/step - loss: 0.0904 - binary_accuracy: 0.9825\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 1s 34us/step - loss: 0.0885 - binary_accuracy: 0.9827\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 1s 35us/step - loss: 0.0871 - binary_accuracy: 0.9829\n",
      "6118/6118 [==============================] - 2s 288us/step\n",
      "For attack1: The loss is 0.09065832566419978 , the accuracy is 0.9810395554492348\n",
      "********For the activation function linear ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 5s 208us/step - loss: 4.5293 - binary_accuracy: 0.2943\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 1s 34us/step - loss: 4.6331 - binary_accuracy: 0.1524\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 1s 33us/step - loss: 4.5735 - binary_accuracy: 0.2015\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 1s 34us/step - loss: 4.5869 - binary_accuracy: 0.1973\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 1s 33us/step - loss: 4.5855 - binary_accuracy: 0.2049\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 1s 34us/step - loss: 4.5825 - binary_accuracy: 0.2118\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 1s 34us/step - loss: 4.5810 - binary_accuracy: 0.2259\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 1s 35us/step - loss: 4.0449 - binary_accuracy: 0.3604\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 1s 33us/step - loss: 4.4036 - binary_accuracy: 0.2605\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 1s 34us/step - loss: 2.8525 - binary_accuracy: 0.1859\n",
      "6118/6118 [==============================] - 2s 295us/step\n",
      "For attack1: The loss is 4.058997022205414 , the accuracy is 0.05867930698619827\n"
     ]
    }
   ],
   "source": [
    "activation_func_use = activation_func[0]\n",
    "loss_func_use = loss_func[0]\n",
    "optimizer_scheme_use = optimizer_scheme[0]\n",
    "\n",
    "for i in range(len(activation_func)):\n",
    "    activation_func_use = activation_func[i]\n",
    "    print('********For the activation function', activation_func[i], '********')\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=n, activation=activation_func_use))\n",
    "    model.add(Dense(16, activation=activation_func_use))\n",
    "    model.add(Dense(1, activation=activation_func_use))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, )\n",
    "\n",
    "    model.compile(loss=loss_func_use, optimizer=optimizer_scheme_use, metrics=['binary_accuracy'])\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=100)\n",
    "\n",
    "    loss_and_metrics = model.evaluate(X_test, y_test, batch_size=128)\n",
    "\n",
    "    print('For attack1: The loss is', loss_and_metrics[0], ', the accuracy is', loss_and_metrics[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that only the sigmoid function and hard_sigmoid function are valid for this kind of data set.\n",
    "We thus choose the Sigmoid funcion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********For the loss function binary_crossentropy ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 2s 77us/step - loss: 0.2529 - binary_accuracy: 0.9245\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.1229 - binary_accuracy: 0.9658\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 16us/step - loss: 0.0939 - binary_accuracy: 0.9756\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.0811 - binary_accuracy: 0.9831\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.0730 - binary_accuracy: 0.9843\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.0683 - binary_accuracy: 0.9851\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.0646 - binary_accuracy: 0.9862\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 16us/step - loss: 0.0624 - binary_accuracy: 0.9863\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.0606 - binary_accuracy: 0.9866\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.0594 - binary_accuracy: 0.9869\n",
      "6118/6118 [==============================] - 1s 102us/step\n",
      "For attack1: The loss is 0.06463440688130279 , the accuracy is 0.9861065702876377\n",
      "********For the loss function mean_squared_error ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 2s 74us/step - loss: 0.0829 - binary_accuracy: 0.9189\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.0377 - binary_accuracy: 0.9607\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.0271 - binary_accuracy: 0.9708\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.0225 - binary_accuracy: 0.9826\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.0196 - binary_accuracy: 0.9844\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.0178 - binary_accuracy: 0.9848\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.0163 - binary_accuracy: 0.9855\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.0154 - binary_accuracy: 0.9862\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.0148 - binary_accuracy: 0.9863\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.0142 - binary_accuracy: 0.9866\n",
      "6118/6118 [==============================] - 1s 101us/step\n",
      "For attack1: The loss is 0.014294096382579 , the accuracy is 0.9867603792088918\n",
      "********For the loss function mean_absolute_error ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 2s 75us/step - loss: 0.2238 - binary_accuracy: 0.8394\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.0977 - binary_accuracy: 0.9367\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.0751 - binary_accuracy: 0.9470\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.0559 - binary_accuracy: 0.9678\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.0462 - binary_accuracy: 0.9714\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.0409 - binary_accuracy: 0.9724\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.0347 - binary_accuracy: 0.9786\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.0294 - binary_accuracy: 0.9863\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.0257 - binary_accuracy: 0.9873\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.0232 - binary_accuracy: 0.9877\n",
      "6118/6118 [==============================] - 1s 104us/step\n",
      "For attack1: The loss is 0.020787775666453248 , the accuracy is 0.9888852566198104\n",
      "********For the loss function mean_absolute_percentage_error ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 2s 77us/step - loss: 48768117.8995 - binary_accuracy: 0.6850\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 14545165.7585 - binary_accuracy: 0.6863\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 8591058.3854 - binary_accuracy: 0.6863\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 6200795.7773 - binary_accuracy: 0.6863\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 4884323.7998 - binary_accuracy: 0.6863\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 4040233.8426 - binary_accuracy: 0.6863\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 3449924.1015 - binary_accuracy: 0.6863\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 3012639.1090 - binary_accuracy: 0.6863\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 2675333.2721 - binary_accuracy: 0.6863\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 2406828.5006 - binary_accuracy: 0.6863\n",
      "6118/6118 [==============================] - 1s 105us/step\n",
      "For attack1: The loss is 2279680.115642367 , the accuracy is 0.6830663618678242\n",
      "********For the loss function mean_squared_logarithmic_error ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 2s 80us/step - loss: 0.0357 - binary_accuracy: 0.9172\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 16us/step - loss: 0.0165 - binary_accuracy: 0.9611\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.0124 - binary_accuracy: 0.9694\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.0103 - binary_accuracy: 0.9747\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.0092 - binary_accuracy: 0.9837\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.0083 - binary_accuracy: 0.9848\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.0077 - binary_accuracy: 0.9851\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.0072 - binary_accuracy: 0.9859: 0s - loss: 0.0072 - binary_accuracy: 0.98\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.0068 - binary_accuracy: 0.9866\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.0066 - binary_accuracy: 0.9868\n",
      "6118/6118 [==============================] - 1s 113us/step\n",
      "For attack1: The loss is 0.0072430011063714305 , the accuracy is 0.984472049708926\n",
      "********For the loss function squared_hinge ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 2s 82us/step - loss: 0.6884 - binary_accuracy: 0.3137\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.6866 - binary_accuracy: 0.3137\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.6864 - binary_accuracy: 0.3137\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.6864 - binary_accuracy: 0.3137\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.6864 - binary_accuracy: 0.3137\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.6864 - binary_accuracy: 0.3137\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.6863 - binary_accuracy: 0.3137\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.6863 - binary_accuracy: 0.3137\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.6863 - binary_accuracy: 0.3137\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.6863 - binary_accuracy: 0.3137\n",
      "6118/6118 [==============================] - 1s 107us/step\n",
      "For attack1: The loss is 0.6832758119270746 , the accuracy is 0.31677018644257127\n",
      "********For the loss function hinge ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 2s 81us/step - loss: 0.6999 - binary_accuracy: 0.3128\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.6913 - binary_accuracy: 0.3128\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.6900 - binary_accuracy: 0.3128\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.6894 - binary_accuracy: 0.3128\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.6890 - binary_accuracy: 0.3128\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.6887 - binary_accuracy: 0.3128\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.6885 - binary_accuracy: 0.3128\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.6883 - binary_accuracy: 0.3128\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.6882 - binary_accuracy: 0.3128\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.6881 - binary_accuracy: 0.3128\n",
      "6118/6118 [==============================] - 1s 117us/step\n",
      "For attack1: The loss is 0.6801775260936517 , the accuracy is 0.3206930370960566\n",
      "********For the loss function logcosh ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 2s 83us/step - loss: 0.0340 - binary_accuracy: 0.9250\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.0166 - binary_accuracy: 0.9615\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.0119 - binary_accuracy: 0.9718\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.0098 - binary_accuracy: 0.9836\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.0085 - binary_accuracy: 0.9848\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.0077 - binary_accuracy: 0.9851\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 16us/step - loss: 0.0072 - binary_accuracy: 0.9858\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.0067 - binary_accuracy: 0.9863\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.0063 - binary_accuracy: 0.9870\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.0062 - binary_accuracy: 0.9868\n",
      "6118/6118 [==============================] - 1s 113us/step\n",
      "For attack1: The loss is 0.006217868732288175 , the accuracy is 0.9870872834259562\n",
      "********For the loss function kullback_leibler_divergence ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 2s 83us/step - loss: 0.0143 - binary_accuracy: 0.3119\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.0042 - binary_accuracy: 0.3107\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.0029 - binary_accuracy: 0.3107\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.0023 - binary_accuracy: 0.3107\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.0019 - binary_accuracy: 0.3107\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 16us/step - loss: 0.0016 - binary_accuracy: 0.3107\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.0014 - binary_accuracy: 0.3107\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.0012 - binary_accuracy: 0.3107\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.0011 - binary_accuracy: 0.3107\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.0010 - binary_accuracy: 0.3107\n",
      "6118/6118 [==============================] - 1s 112us/step\n",
      "For attack1: The loss is 0.001012975554082125 , the accuracy is 0.3288656425129241\n",
      "********For the loss function poisson ********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 2s 85us/step - loss: 0.4580 - binary_accuracy: 0.9181\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.3859 - binary_accuracy: 0.9592\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 16us/step - loss: 0.3688 - binary_accuracy: 0.9686\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.3627 - binary_accuracy: 0.9731\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.3592 - binary_accuracy: 0.9824\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.3568 - binary_accuracy: 0.9839\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.3553 - binary_accuracy: 0.9843\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.3555 - binary_accuracy: 0.9838\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 16us/step - loss: 0.3535 - binary_accuracy: 0.9848\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 16us/step - loss: 0.3529 - binary_accuracy: 0.9851\n",
      "6118/6118 [==============================] - 1s 123us/step\n",
      "For attack1: The loss is 0.36656623066504357 , the accuracy is 0.9805491986170279\n",
      "********For the loss function cosine_proximity ********\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3003: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 2s 86us/step - loss: -0.3153 - binary_accuracy: 0.3153\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 16us/step - loss: -0.3153 - binary_accuracy: 0.3153\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: -0.3153 - binary_accuracy: 0.3153\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: -0.3153 - binary_accuracy: 0.3153\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: -0.3153 - binary_accuracy: 0.3153\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: -0.3153 - binary_accuracy: 0.3153\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: -0.3153 - binary_accuracy: 0.3153\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: -0.3153 - binary_accuracy: 0.3153\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: -0.3153 - binary_accuracy: 0.3153\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: -0.3153 - binary_accuracy: 0.3153\n",
      "6118/6118 [==============================] - 1s 119us/step\n",
      "For attack1: The loss is -0.3105589645522293 , the accuracy is 0.3105590063865452\n"
     ]
    }
   ],
   "source": [
    "activation_func_use = activation_func[0]\n",
    "loss_func_use = loss_func[0]\n",
    "optimizer_scheme_use = optimizer_scheme[0]\n",
    "\n",
    "for i in range(len(loss_func)):\n",
    "    loss_func_use = loss_func[i]\n",
    "    print('********For the loss function', loss_func[i], '********')\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=n, activation=activation_func_use))\n",
    "    model.add(Dense(16, activation=activation_func_use))\n",
    "    model.add(Dense(1, activation=activation_func_use))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, )\n",
    "\n",
    "    model.compile(loss=loss_func_use, optimizer=optimizer_scheme_use, metrics=['binary_accuracy'])\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=100)\n",
    "\n",
    "    loss_and_metrics = model.evaluate(X_test, y_test, batch_size=128)\n",
    "\n",
    "    print('For attack1: The loss is', loss_and_metrics[0], ', the accuracy is', loss_and_metrics[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that many loss functions are effective for this optimization problem. \n",
    "So we choose the 'mean_absolute_error' as the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the proper Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********For the optimizer_scheme Adagrad ***********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 2s 91us/step - loss: 0.3015 - binary_accuracy: 0.9137\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 16us/step - loss: 0.1538 - binary_accuracy: 0.9564\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 16us/step - loss: 0.1111 - binary_accuracy: 0.9680\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 16us/step - loss: 0.0920 - binary_accuracy: 0.9788\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 16us/step - loss: 0.0812 - binary_accuracy: 0.9844\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 16us/step - loss: 0.0748 - binary_accuracy: 0.9848\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 16us/step - loss: 0.0698 - binary_accuracy: 0.9850\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 17us/step - loss: 0.0666 - binary_accuracy: 0.9853\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 16us/step - loss: 0.0644 - binary_accuracy: 0.9860\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 17us/step - loss: 0.0619 - binary_accuracy: 0.9863\n",
      "6118/6118 [==============================] - 1s 120us/step\n",
      "For attack1: The loss is 0.06400303794214171 , the accuracy is 0.9844720492218008\n",
      "***********For the optimizer_scheme SGD ***********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 2s 91us/step - loss: 0.5831 - binary_accuracy: 0.6862\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 16us/step - loss: 0.5206 - binary_accuracy: 0.6862\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 16us/step - loss: 0.4651 - binary_accuracy: 0.7894\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 16us/step - loss: 0.4193 - binary_accuracy: 0.8986\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 16us/step - loss: 0.3820 - binary_accuracy: 0.9055\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.3510 - binary_accuracy: 0.9139\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 16us/step - loss: 0.3250 - binary_accuracy: 0.9188\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.3041 - binary_accuracy: 0.9232\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 15us/step - loss: 0.2866 - binary_accuracy: 0.9280\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 16us/step - loss: 0.2721 - binary_accuracy: 0.9298\n",
      "6118/6118 [==============================] - 1s 124us/step\n",
      "For attack1: The loss is 0.2556673607753359 , the accuracy is 0.9355998687706729\n",
      "***********For the optimizer_scheme RMSprop ***********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 2s 93us/step - loss: 0.3196 - binary_accuracy: 0.8977\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 16us/step - loss: 0.1229 - binary_accuracy: 0.9611\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 16us/step - loss: 0.0768 - binary_accuracy: 0.9816\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 16us/step - loss: 0.0653 - binary_accuracy: 0.9841\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 16us/step - loss: 0.0614 - binary_accuracy: 0.9850\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 17us/step - loss: 0.0581 - binary_accuracy: 0.9858\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 16us/step - loss: 0.0562 - binary_accuracy: 0.9864\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 16us/step - loss: 0.0532 - binary_accuracy: 0.9868\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 16us/step - loss: 0.0515 - binary_accuracy: 0.9871\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 17us/step - loss: 0.0493 - binary_accuracy: 0.9876\n",
      "6118/6118 [==============================] - 1s 126us/step\n",
      "For attack1: The loss is 0.04279050316592181 , the accuracy is 0.9888852561326852\n",
      "***********For the optimizer_scheme Adadelta ***********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 2s 99us/step - loss: 0.6976 - binary_accuracy: 0.6461\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 1s 31us/step - loss: 0.3961 - binary_accuracy: 0.9193\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 1s 33us/step - loss: 0.2675 - binary_accuracy: 0.9336\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 1s 33us/step - loss: 0.2040 - binary_accuracy: 0.9403\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 1s 32us/step - loss: 0.1459 - binary_accuracy: 0.9585\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 1s 33us/step - loss: 0.1030 - binary_accuracy: 0.9722\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 1s 34us/step - loss: 0.0819 - binary_accuracy: 0.9793\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 1s 32us/step - loss: 0.0702 - binary_accuracy: 0.9855\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 1s 36us/step - loss: 0.0630 - binary_accuracy: 0.9859\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 1s 37us/step - loss: 0.0586 - binary_accuracy: 0.9866\n",
      "6118/6118 [==============================] - 1s 229us/step\n",
      "For attack1: The loss is 0.06580666823957472 , the accuracy is 0.984472049689441\n",
      "***********For the optimizer_scheme Adam ***********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 2s 99us/step - loss: 0.3840 - binary_accuracy: 0.8562\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 18us/step - loss: 0.1421 - binary_accuracy: 0.9601\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 18us/step - loss: 0.0861 - binary_accuracy: 0.9821\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 18us/step - loss: 0.0711 - binary_accuracy: 0.9843\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 19us/step - loss: 0.0652 - binary_accuracy: 0.9850\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 19us/step - loss: 0.0598 - binary_accuracy: 0.9861\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 18us/step - loss: 0.0563 - binary_accuracy: 0.9869\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 0s 19us/step - loss: 0.0574 - binary_accuracy: 0.9861\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 0s 19us/step - loss: 0.0545 - binary_accuracy: 0.9869\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 0s 20us/step - loss: 0.0530 - binary_accuracy: 0.9871\n",
      "6118/6118 [==============================] - 1s 176us/step\n",
      "For attack1: The loss is 0.04695786542072527 , the accuracy is 0.9892121603692345\n",
      "***********For the optimizer_scheme Adamax ***********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 4s 181us/step - loss: 0.3242 - binary_accuracy: 0.8921\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 1s 39us/step - loss: 0.1429 - binary_accuracy: 0.9564\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 1s 40us/step - loss: 0.0858 - binary_accuracy: 0.9808\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 1s 43us/step - loss: 0.0696 - binary_accuracy: 0.9839\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 1s 40us/step - loss: 0.0622 - binary_accuracy: 0.9850\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 1s 39us/step - loss: 0.0580 - binary_accuracy: 0.9863\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 1s 40us/step - loss: 0.0559 - binary_accuracy: 0.9867\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 1s 33us/step - loss: 0.0541 - binary_accuracy: 0.9872\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 1s 33us/step - loss: 0.0529 - binary_accuracy: 0.9874\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 1s 32us/step - loss: 0.0514 - binary_accuracy: 0.9877\n",
      "6118/6118 [==============================] - 2s 246us/step\n",
      "For attack1: The loss is 0.05320551450449234 , the accuracy is 0.987087282938831\n",
      "***********For the optimizer_scheme Nadam ***********\n",
      "Epoch 1/10\n",
      "24470/24470 [==============================] - 5s 187us/step - loss: 0.2521 - binary_accuracy: 0.9231\n",
      "Epoch 2/10\n",
      "24470/24470 [==============================] - 0s 19us/step - loss: 0.0848 - binary_accuracy: 0.9787\n",
      "Epoch 3/10\n",
      "24470/24470 [==============================] - 0s 20us/step - loss: 0.0684 - binary_accuracy: 0.9827\n",
      "Epoch 4/10\n",
      "24470/24470 [==============================] - 0s 18us/step - loss: 0.0617 - binary_accuracy: 0.9852\n",
      "Epoch 5/10\n",
      "24470/24470 [==============================] - 0s 19us/step - loss: 0.0543 - binary_accuracy: 0.9877\n",
      "Epoch 6/10\n",
      "24470/24470 [==============================] - 0s 19us/step - loss: 0.0575 - binary_accuracy: 0.9867\n",
      "Epoch 7/10\n",
      "24470/24470 [==============================] - 0s 18us/step - loss: 0.0484 - binary_accuracy: 0.9885\n",
      "Epoch 8/10\n",
      "24470/24470 [==============================] - 1s 29us/step - loss: 0.0588 - binary_accuracy: 0.9841\n",
      "Epoch 9/10\n",
      "24470/24470 [==============================] - 1s 43us/step - loss: 0.0486 - binary_accuracy: 0.9866\n",
      "Epoch 10/10\n",
      "24470/24470 [==============================] - 1s 44us/step - loss: 0.0426 - binary_accuracy: 0.9887\n",
      "6118/6118 [==============================] - 2s 255us/step\n",
      "For attack1: The loss is 0.04488159225107058 , the accuracy is 0.987904543500977\n"
     ]
    }
   ],
   "source": [
    "activation_func_use = activation_func[0]\n",
    "loss_func_use = loss_func[0]\n",
    "optimizer_scheme_use = optimizer_scheme[0]\n",
    "\n",
    "for i in range(len(optimizer_scheme)):\n",
    "    optimizer_scheme_use = optimizer_scheme[i]\n",
    "    print('***********For the optimizer_scheme', optimizer_scheme[i], '***********')\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=n, activation=activation_func_use))\n",
    "    model.add(Dense(16, activation=activation_func_use))\n",
    "    model.add(Dense(1, activation=activation_func_use))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, )\n",
    "\n",
    "    model.compile(loss=loss_func_use, optimizer=optimizer_scheme_use, metrics=['binary_accuracy'])\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=100)\n",
    "\n",
    "    loss_and_metrics = model.evaluate(X_test, y_test, batch_size=128)\n",
    "\n",
    "    print('For attack1: The loss is', loss_and_metrics[0], ', the accuracy is', loss_and_metrics[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that besides from SGD, the others are quite efficient. \n",
    "\n",
    "Considering the setup speed, we choose the RMSprop as the Optimizer scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_func_use = activation_func[0] #'sigmoid'\n",
    "loss_func_use = loss_func[2] #'mean_absolute_error'\n",
    "optimizer_scheme_use = optimizer_scheme[2] #'RMSprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_predict, y_test):\n",
    "    count_ccr = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_predict[i]==y_test[i]:\n",
    "            count_ccr+=1\n",
    "        if y_predict[i]==1 and y_test[i]==1:\n",
    "            TP+=1\n",
    "        if y_predict[i]==1 and y_test[i]==0:\n",
    "            FP+=1\n",
    "        if y_predict[i]==0 and y_test[i]==1:\n",
    "            FN+=1\n",
    "    ccr = count_ccr/len(y_test)\n",
    "    if (TP+FP)==0:\n",
    "        print('All the prediction is normal')\n",
    "        preci = 0\n",
    "    else:\n",
    "        preci = TP/(TP+FP)\n",
    "    recall= TP/(TP+FN)\n",
    "    print('For this model, the CCR is', ccr, ', the Precision is', preci, 'and the Recall is', recall )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "24470/24470 [==============================] - 1s 27us/step - loss: 0.2251 - binary_accuracy: 0.8389\n",
      "Epoch 2/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0767 - binary_accuracy: 0.9390\n",
      "Epoch 3/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0446 - binary_accuracy: 0.9635\n",
      "Epoch 4/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0263 - binary_accuracy: 0.9804\n",
      "Epoch 5/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0170 - binary_accuracy: 0.9869\n",
      "Epoch 6/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0142 - binary_accuracy: 0.9872\n",
      "Epoch 7/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0130 - binary_accuracy: 0.9877\n",
      "Epoch 8/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0125 - binary_accuracy: 0.9878\n",
      "Epoch 9/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0118 - binary_accuracy: 0.9887\n",
      "Epoch 10/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0122 - binary_accuracy: 0.9880\n",
      "Epoch 11/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0116 - binary_accuracy: 0.9886\n",
      "Epoch 12/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0119 - binary_accuracy: 0.9882\n",
      "Epoch 13/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0112 - binary_accuracy: 0.9889\n",
      "Epoch 14/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0112 - binary_accuracy: 0.9888\n",
      "Epoch 15/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0115 - binary_accuracy: 0.9886\n",
      "Epoch 16/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0114 - binary_accuracy: 0.9886\n",
      "Epoch 17/20\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.0109 - binary_accuracy: 0.9892\n",
      "Epoch 18/20\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.0112 - binary_accuracy: 0.9888\n",
      "Epoch 19/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0114 - binary_accuracy: 0.9886\n",
      "Epoch 20/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.0109 - binary_accuracy: 0.9891\n",
      "For this model, the CCR is 0.9880679960771493 , the Precision is 0.9994853319608852 and the Recall is 0.9642502482621649\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=n, activation=activation_func_use))\n",
    "model.add(Dense(16, activation=activation_func_use))\n",
    "model.add(Dense(1, activation=activation_func_use))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, )\n",
    "\n",
    "model.compile(loss=loss_func_use, optimizer=optimizer_scheme_use, metrics=['binary_accuracy'])\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=100)\n",
    "y_predict = np.round(model.predict(X_test))\n",
    "evaluate_model(y_predict, y_test)\n",
    "\n",
    "#print('For attack1: The loss is', loss_and_metrics[0], ', the accuracy is', loss_and_metrics[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9883949])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "1- sum(abs(np.round(model.predict(X_test))-y_test))/len(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3143389564535112"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_attack2 = pd.read_csv(\"dataset/attack2with7FeatureVector.csv\")\n",
    "data_attack2 = data_attack2.dropna(axis=0, how=\"any\")#remove invalid data\n",
    "data_attack2['Label'] = data_attack2['Label']/2\n",
    "sum(np.array(data_attack2['Label']))/data_attack2['Label'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "24470/24470 [==============================] - 1s 31us/step - loss: 0.3387 - binary_accuracy: 0.6874\n",
      "Epoch 2/20\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.3138 - binary_accuracy: 0.6874\n",
      "Epoch 3/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.3127 - binary_accuracy: 0.6874\n",
      "Epoch 4/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.3126 - binary_accuracy: 0.6874\n",
      "Epoch 5/20\n",
      "24470/24470 [==============================] - 0s 14us/step - loss: 0.3126 - binary_accuracy: 0.6874\n",
      "Epoch 6/20\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.3126 - binary_accuracy: 0.6874\n",
      "Epoch 7/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.3126 - binary_accuracy: 0.6874\n",
      "Epoch 8/20\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.3126 - binary_accuracy: 0.6874\n",
      "Epoch 9/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.3126 - binary_accuracy: 0.6874\n",
      "Epoch 10/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.3126 - binary_accuracy: 0.6874\n",
      "Epoch 11/20\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.3126 - binary_accuracy: 0.6874\n",
      "Epoch 12/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.3126 - binary_accuracy: 0.6874\n",
      "Epoch 13/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.3126 - binary_accuracy: 0.6874\n",
      "Epoch 14/20\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.3126 - binary_accuracy: 0.6874\n",
      "Epoch 15/20\n",
      "24470/24470 [==============================] - 0s 13us/step - loss: 0.3126 - binary_accuracy: 0.6874\n",
      "Epoch 16/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.3126 - binary_accuracy: 0.6874\n",
      "Epoch 17/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.3126 - binary_accuracy: 0.6874\n",
      "Epoch 18/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.3126 - binary_accuracy: 0.6874\n",
      "Epoch 19/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.3126 - binary_accuracy: 0.6874\n",
      "Epoch 20/20\n",
      "24470/24470 [==============================] - 0s 12us/step - loss: 0.3126 - binary_accuracy: 0.6874\n",
      "All the prediction is normal\n",
      "For this model, the CCR is 0.6788166067342268 , the Precision is 0 and the Recall is 0.0\n"
     ]
    }
   ],
   "source": [
    "X = data_attack2.iloc[:,12:] \n",
    "n = X.shape[1]\n",
    "y = data_attack2.iloc[:,11]\n",
    "X = np.reshape(X.values, (X.shape[0], X.shape[1]))\n",
    "y = np.reshape(y.values, (y.shape[0],  1))\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(64, input_dim=n, activation=activation_func_use))\n",
    "model2.add(Dense(16, activation=activation_func_use))\n",
    "model2.add(Dense(1, activation=activation_func_use))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, )\n",
    "\n",
    "model2.compile(loss=loss_func_use, optimizer=optimizer_scheme_use, metrics=['binary_accuracy'])\n",
    "model2.fit(X_train, y_train, epochs=20, batch_size=100)\n",
    "y_predict = np.round(model2.predict(X_test))\n",
    "\n",
    "evaluate_model(y_predict, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3448534690947066"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_attack4 = pd.read_csv(\"dataset/attack4with7FeatureVector.csv\")\n",
    "data_attack4 = data_attack4.dropna(axis=0, how=\"any\")#remove invalid data\n",
    "data_attack4['Label'] = data_attack4['Label']/4\n",
    "sum(np.array(data_attack4['Label']))/data_attack4['Label'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "24513/24513 [==============================] - 1s 33us/step - loss: 0.2739 - binary_accuracy: 0.8156\n",
      "Epoch 2/10\n",
      "24513/24513 [==============================] - 0s 13us/step - loss: 0.0951 - binary_accuracy: 0.9262\n",
      "Epoch 3/10\n",
      "24513/24513 [==============================] - 0s 13us/step - loss: 0.0427 - binary_accuracy: 0.9622\n",
      "Epoch 4/10\n",
      "24513/24513 [==============================] - 0s 13us/step - loss: 0.0390 - binary_accuracy: 0.9622\n",
      "Epoch 5/10\n",
      "24513/24513 [==============================] - 0s 13us/step - loss: 0.0384 - binary_accuracy: 0.9621\n",
      "Epoch 6/10\n",
      "24513/24513 [==============================] - 0s 12us/step - loss: 0.0383 - binary_accuracy: 0.9619\n",
      "Epoch 7/10\n",
      "24513/24513 [==============================] - 0s 12us/step - loss: 0.0370 - binary_accuracy: 0.9634\n",
      "Epoch 8/10\n",
      "24513/24513 [==============================] - 0s 13us/step - loss: 0.0372 - binary_accuracy: 0.9630\n",
      "Epoch 9/10\n",
      "24513/24513 [==============================] - 0s 13us/step - loss: 0.0373 - binary_accuracy: 0.9629\n",
      "Epoch 10/10\n",
      "24513/24513 [==============================] - 0s 12us/step - loss: 0.0374 - binary_accuracy: 0.9626\n",
      "For this model, the CCR is 0.9647577092511013 , the Precision is 0.9978575254418853 and the Recall is 0.8978313253012048\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = data_attack4.iloc[:,12:] \n",
    "n = X.shape[1]\n",
    "y = data_attack4.iloc[:,11]\n",
    "X = np.reshape(X.values, (X.shape[0], X.shape[1]))\n",
    "y =np.reshape(y.values, (y.shape[0],  1))\n",
    "\n",
    "model4 = Sequential()\n",
    "model4.add(Dense(64, input_dim=n, activation=activation_func_use))\n",
    "model4.add(Dense(16, activation=activation_func_use))\n",
    "model4.add(Dense(1, activation=activation_func_use))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, )\n",
    "\n",
    "model4.compile(loss=loss_func_use, optimizer=optimizer_scheme_use, metrics=['binary_accuracy'])\n",
    "model4.fit(X_train, y_train, epochs=10, batch_size=100)\n",
    "y_predict = np.round(model4.predict(X_test))\n",
    "\n",
    "evaluate_model(y_predict, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2550987057131651"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_attack8 = pd.read_csv(\"dataset/attack8with7FeatureVector.csv\")\n",
    "data_attack8 = data_attack8.dropna(axis=0, how=\"any\")#remove invalid data\n",
    "data_attack8['Label'] = data_attack8['Label']/8\n",
    "sum(np.array(data_attack8['Label']))/data_attack8['Label'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "24476/24476 [==============================] - 1s 34us/step - loss: 0.2685 - binary_accuracy: 0.7897\n",
      "Epoch 2/10\n",
      "24476/24476 [==============================] - 0s 13us/step - loss: 0.0774 - binary_accuracy: 0.9457\n",
      "Epoch 3/10\n",
      "24476/24476 [==============================] - 0s 13us/step - loss: 0.0467 - binary_accuracy: 0.9609\n",
      "Epoch 4/10\n",
      "24476/24476 [==============================] - 0s 12us/step - loss: 0.0350 - binary_accuracy: 0.9685\n",
      "Epoch 5/10\n",
      "24476/24476 [==============================] - 0s 12us/step - loss: 0.0320 - binary_accuracy: 0.9697\n",
      "Epoch 6/10\n",
      "24476/24476 [==============================] - 0s 12us/step - loss: 0.0306 - binary_accuracy: 0.9709\n",
      "Epoch 7/10\n",
      "24476/24476 [==============================] - 0s 12us/step - loss: 0.0301 - binary_accuracy: 0.9708\n",
      "Epoch 8/10\n",
      "24476/24476 [==============================] - 0s 12us/step - loss: 0.0302 - binary_accuracy: 0.9704\n",
      "Epoch 9/10\n",
      "24476/24476 [==============================] - 0s 12us/step - loss: 0.0295 - binary_accuracy: 0.9715\n",
      "Epoch 10/10\n",
      "24476/24476 [==============================] - 0s 12us/step - loss: 0.0290 - binary_accuracy: 0.9719\n",
      "For this model, the CCR is 0.9758169934640523 , the Precision is 0.9986338797814208 and the Recall is 0.9092039800995025\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = data_attack8.iloc[:,12:] \n",
    "n = X.shape[1]\n",
    "y = data_attack8.iloc[:,11]\n",
    "X = np.reshape(X.values, (X.shape[0], X.shape[1]))\n",
    "y =np.reshape(y.values, (y.shape[0],  1))\n",
    "\n",
    "model8 = Sequential()\n",
    "model8.add(Dense(64, input_dim=n, activation=activation_func_use))\n",
    "model8.add(Dense(16, activation=activation_func_use))\n",
    "model8.add(Dense(1, activation=activation_func_use))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, )\n",
    "\n",
    "model8.compile(loss=loss_func_use, optimizer=optimizer_scheme_use, metrics=['binary_accuracy'])\n",
    "model8.fit(X_train, y_train, epochs=10, batch_size=100)\n",
    "y_predict = np.round(model8.predict(X_test))\n",
    "\n",
    "evaluate_model(y_predict, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2963955426293258"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_attack16 = pd.read_csv(\"dataset/attack16with7FeatureVector.csv\")\n",
    "data_attack16 = data_attack16.dropna(axis=0, how=\"any\")#remove invalid data\n",
    "data_attack16['Label'] = data_attack16['Label']/16\n",
    "sum(np.array(data_attack16['Label']))/data_attack16['Label'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "24480/24480 [==============================] - 1s 35us/step - loss: 0.2321 - binary_accuracy: 0.8357\n",
      "Epoch 2/10\n",
      "24480/24480 [==============================] - 0s 12us/step - loss: 0.0825 - binary_accuracy: 0.9319\n",
      "Epoch 3/10\n",
      "24480/24480 [==============================] - 0s 12us/step - loss: 0.0653 - binary_accuracy: 0.9383\n",
      "Epoch 4/10\n",
      "24480/24480 [==============================] - 0s 12us/step - loss: 0.0509 - binary_accuracy: 0.9533\n",
      "Epoch 5/10\n",
      "24480/24480 [==============================] - 0s 12us/step - loss: 0.0460 - binary_accuracy: 0.9570\n",
      "Epoch 6/10\n",
      "24480/24480 [==============================] - 0s 12us/step - loss: 0.0406 - binary_accuracy: 0.9618\n",
      "Epoch 7/10\n",
      "24480/24480 [==============================] - 0s 12us/step - loss: 0.0387 - binary_accuracy: 0.9638\n",
      "Epoch 8/10\n",
      "24480/24480 [==============================] - 0s 12us/step - loss: 0.0374 - binary_accuracy: 0.9644\n",
      "Epoch 9/10\n",
      "24480/24480 [==============================] - 0s 12us/step - loss: 0.0362 - binary_accuracy: 0.9652\n",
      "Epoch 10/10\n",
      "24480/24480 [==============================] - 0s 12us/step - loss: 0.0358 - binary_accuracy: 0.9655\n",
      "For this model, the CCR is 0.9671622283940533 , the Precision is 0.9969806763285024 and the Recall is 0.8938819707634001\n"
     ]
    }
   ],
   "source": [
    "X = data_attack16.iloc[:,12:] \n",
    "n = X.shape[1]\n",
    "y = data_attack16.iloc[:,11]\n",
    "X = np.reshape(X.values, (X.shape[0], X.shape[1]))\n",
    "y =np.reshape(y.values, (y.shape[0],  1))\n",
    "\n",
    "model16 = Sequential()\n",
    "model16.add(Dense(64, input_dim=n, activation=activation_func_use))\n",
    "model16.add(Dense(16, activation=activation_func_use))\n",
    "model16.add(Dense(1, activation=activation_func_use))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, )\n",
    "\n",
    "model16.compile(loss=loss_func_use, optimizer=optimizer_scheme_use, metrics=['binary_accuracy'])\n",
    "model16.fit(X_train, y_train, epochs=10, batch_size=100)\n",
    "y_predict = np.round(model16.predict(X_test))\n",
    "\n",
    "evaluate_model(y_predict, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_overall = data_attack1.append([data_attack2, data_attack4, data_attack8, data_attack16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153015, 19)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_overall.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "122412/122412 [==============================] - 1s 11us/step - loss: 0.2029 - binary_accuracy: 0.8361\n",
      "Epoch 2/20\n",
      "122412/122412 [==============================] - 1s 6us/step - loss: 0.0904 - binary_accuracy: 0.9138\n",
      "Epoch 3/20\n",
      "122412/122412 [==============================] - 1s 6us/step - loss: 0.0800 - binary_accuracy: 0.9212\n",
      "Epoch 4/20\n",
      "122412/122412 [==============================] - 1s 6us/step - loss: 0.0784 - binary_accuracy: 0.9224\n",
      "Epoch 5/20\n",
      "122412/122412 [==============================] - 1s 6us/step - loss: 0.0778 - binary_accuracy: 0.9227\n",
      "Epoch 6/20\n",
      "122412/122412 [==============================] - 1s 6us/step - loss: 0.0771 - binary_accuracy: 0.9233\n",
      "Epoch 7/20\n",
      "122412/122412 [==============================] - 1s 6us/step - loss: 0.0774 - binary_accuracy: 0.9229\n",
      "Epoch 8/20\n",
      "122412/122412 [==============================] - 1s 6us/step - loss: 0.0771 - binary_accuracy: 0.9232\n",
      "Epoch 9/20\n",
      "122412/122412 [==============================] - 1s 7us/step - loss: 0.0768 - binary_accuracy: 0.9235\n",
      "Epoch 10/20\n",
      "122412/122412 [==============================] - 1s 7us/step - loss: 0.0769 - binary_accuracy: 0.9234\n",
      "Epoch 11/20\n",
      "122412/122412 [==============================] - 1s 6us/step - loss: 0.0766 - binary_accuracy: 0.9236\n",
      "Epoch 12/20\n",
      "122412/122412 [==============================] - 1s 7us/step - loss: 0.0766 - binary_accuracy: 0.9236\n",
      "Epoch 13/20\n",
      "122412/122412 [==============================] - 1s 6us/step - loss: 0.0766 - binary_accuracy: 0.9237\n",
      "Epoch 14/20\n",
      "122412/122412 [==============================] - 1s 6us/step - loss: 0.0764 - binary_accuracy: 0.9238\n",
      "Epoch 15/20\n",
      "122412/122412 [==============================] - 1s 7us/step - loss: 0.0763 - binary_accuracy: 0.9239\n",
      "Epoch 16/20\n",
      "122412/122412 [==============================] - 1s 7us/step - loss: 0.0763 - binary_accuracy: 0.9240\n",
      "Epoch 17/20\n",
      "122412/122412 [==============================] - 1s 6us/step - loss: 0.0764 - binary_accuracy: 0.9238\n",
      "Epoch 18/20\n",
      "122412/122412 [==============================] - 1s 7us/step - loss: 0.0763 - binary_accuracy: 0.9240\n",
      "Epoch 19/20\n",
      "122412/122412 [==============================] - 1s 7us/step - loss: 0.0761 - binary_accuracy: 0.9242\n",
      "Epoch 20/20\n",
      "122412/122412 [==============================] - 1s 7us/step - loss: 0.0762 - binary_accuracy: 0.9240\n",
      "For this model, the CCR is 0.9234715550762997 , the Precision is 0.9958975809874098 and the Recall is 0.7526996685555437\n"
     ]
    }
   ],
   "source": [
    "X = data_overall.iloc[:,12:] \n",
    "n = X.shape[1]\n",
    "y = data_overall.iloc[:,11]\n",
    "X = np.reshape(X.values, (X.shape[0], X.shape[1]))\n",
    "y =np.reshape(y.values, (y.shape[0],  1))\n",
    "\n",
    "model_all = Sequential()\n",
    "model_all.add(Dense(64, input_dim=n, activation=activation_func_use))\n",
    "model_all.add(Dense(16, activation=activation_func_use))\n",
    "model_all.add(Dense(1, activation=activation_func_use))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, )\n",
    "\n",
    "model_all.compile(loss=loss_func_use, optimizer=optimizer_scheme_use, metrics=['binary_accuracy'])\n",
    "model_all.fit(X_train, y_train, epochs=20, batch_size=200)\n",
    "y_predict = np.round(model_all.predict(X_test))\n",
    "\n",
    "evaluate_model(y_predict, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "We find out a set of data to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.feature_each_transmission import location_plausibility, movement_plausibility, quantititative_information, distance_check\n",
    "\n",
    "def get_data(data, mode):\n",
    "    data_category = data.loc[(data['Label']==mode)]\n",
    "    data_sender_ID = np.unique(np.array(data_category.iloc[:,5]))\n",
    "    n = np.random.randint(len(data_sender_ID))\n",
    "    data_sender = data_category.loc[data_category['tr_ID'] == data_sender_ID[n]]\n",
    "    data_recevier_ID = np.unique(np.array(data_sender.iloc[:,1]))\n",
    "    m = np.random.randint(len(data_recevier_ID))\n",
    "    data_select = data_sender.loc[data_sender['re_ID']==data_recevier_ID[m]]\n",
    "    return data_select\n",
    "\n",
    "\n",
    "def test_real_data(model_all, data, mode):\n",
    "    \n",
    "    data_vector =np.zeros((1, 7))\n",
    "    data_vector[0][0] = location_plausibility(data)\n",
    "    data_vector[0][1] = movement_plausibility(data)\n",
    "    data_vector[0][2], data_vector[0][3], data_vector[0][4], data_vector[0][5] =quantititative_information(data)\n",
    "    data_vector[0][6] = distance_check(data,800)\n",
    "    \n",
    "    y_predict = np.round(model_all.predict(data_vector))\n",
    "    \n",
    "    if y_predict==1:\n",
    "        print('The detection system said: The BSM is malicious!')\n",
    "        if mode==1:\n",
    "            print('The detection is correct!')\n",
    "        else:\n",
    "            print('But in fact, the BSM is normal, the detection is incorrect')\n",
    "    if y_predict==0:\n",
    "        print('The detection system said: The BSM is normal!')\n",
    "        if mode==0:\n",
    "            print('The detection is correct!')\n",
    "        else:\n",
    "            print('But in fact, the BSM is malicious, the detection is incorrect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "col_index = [1,2,3,4,6,7,9,10,12,13,15,16]\n",
    "col_names = ['re_time','re_ID','re_x','re_y','tr_time','tr_ID','tr_x','tr_y','tr_vx','tr_vy','RSSI','Label']\n",
    "\n",
    "data = pd.read_csv('dataset/attack1withlabels.csv', usecols = col_index, header = None, names = col_names)\n",
    "data_atk1 = data.dropna(axis = 0, how = 'any')\n",
    "\n",
    "data = pd.read_csv('dataset/attack2withlabels.csv', usecols = col_index, header = None, names = col_names)\n",
    "data_atk2 = data.dropna(axis = 0, how = 'any')\n",
    "data_atk2['Label'] = data_atk2['Label']/2\n",
    "\n",
    "data = pd.read_csv('dataset/attack4withlabels.csv', usecols = col_index, header = None, names = col_names)\n",
    "data_atk4 = data.dropna(axis = 0, how = 'any')\n",
    "data_atk4['Label'] = data_atk16['Label']/4\n",
    "\n",
    "data = pd.read_csv('dataset/attack8withlabels.csv', usecols = col_index, header = None, names = col_names)\n",
    "data_atk8 = data.dropna(axis = 0, how = 'any')\n",
    "data_atk8['Label'] = data_atk8['Label']/8\n",
    "\n",
    "data = pd.read_csv('dataset/attack16withlabels.csv', usecols = col_index, header = None, names = col_names)\n",
    "data_atk16 = data.dropna(axis = 0, how = 'any')\n",
    "data_atk16['Label'] = data_atk16['Label']/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>re_time</th>\n",
       "      <th>re_ID</th>\n",
       "      <th>re_x</th>\n",
       "      <th>re_y</th>\n",
       "      <th>tr_time</th>\n",
       "      <th>tr_ID</th>\n",
       "      <th>tr_x</th>\n",
       "      <th>tr_y</th>\n",
       "      <th>tr_vx</th>\n",
       "      <th>tr_vy</th>\n",
       "      <th>RSSI</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32073</th>\n",
       "      <td>21846</td>\n",
       "      <td>2332</td>\n",
       "      <td>5594.9</td>\n",
       "      <td>5677.9</td>\n",
       "      <td>21846</td>\n",
       "      <td>1855</td>\n",
       "      <td>6336.3</td>\n",
       "      <td>5577.9</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.503500e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32090</th>\n",
       "      <td>21848</td>\n",
       "      <td>2332</td>\n",
       "      <td>5638.7</td>\n",
       "      <td>5683.5</td>\n",
       "      <td>21848</td>\n",
       "      <td>1855</td>\n",
       "      <td>6336.3</td>\n",
       "      <td>5577.9</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.405400e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32106</th>\n",
       "      <td>21850</td>\n",
       "      <td>2332</td>\n",
       "      <td>5682.5</td>\n",
       "      <td>5689.1</td>\n",
       "      <td>21850</td>\n",
       "      <td>1855</td>\n",
       "      <td>6336.3</td>\n",
       "      <td>5577.9</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.292200e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32113</th>\n",
       "      <td>21851</td>\n",
       "      <td>2332</td>\n",
       "      <td>5704.4</td>\n",
       "      <td>5691.9</td>\n",
       "      <td>21851</td>\n",
       "      <td>1855</td>\n",
       "      <td>6336.3</td>\n",
       "      <td>5577.9</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.367800e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32124</th>\n",
       "      <td>21852</td>\n",
       "      <td>2332</td>\n",
       "      <td>5726.1</td>\n",
       "      <td>5694.7</td>\n",
       "      <td>21852</td>\n",
       "      <td>1855</td>\n",
       "      <td>6336.3</td>\n",
       "      <td>5577.9</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.697000e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32150</th>\n",
       "      <td>21854</td>\n",
       "      <td>2332</td>\n",
       "      <td>5764.3</td>\n",
       "      <td>5699.6</td>\n",
       "      <td>21854</td>\n",
       "      <td>1855</td>\n",
       "      <td>6336.3</td>\n",
       "      <td>5577.9</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.217600e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32189</th>\n",
       "      <td>21857</td>\n",
       "      <td>2332</td>\n",
       "      <td>5813.0</td>\n",
       "      <td>5704.2</td>\n",
       "      <td>21857</td>\n",
       "      <td>1855</td>\n",
       "      <td>6336.3</td>\n",
       "      <td>5577.9</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.758700e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32199</th>\n",
       "      <td>21858</td>\n",
       "      <td>2332</td>\n",
       "      <td>5828.5</td>\n",
       "      <td>5705.3</td>\n",
       "      <td>21858</td>\n",
       "      <td>1855</td>\n",
       "      <td>6336.3</td>\n",
       "      <td>5577.9</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.445700e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32212</th>\n",
       "      <td>21859</td>\n",
       "      <td>2332</td>\n",
       "      <td>5843.5</td>\n",
       "      <td>5706.4</td>\n",
       "      <td>21859</td>\n",
       "      <td>1855</td>\n",
       "      <td>6336.3</td>\n",
       "      <td>5577.9</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.930300e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32225</th>\n",
       "      <td>21860</td>\n",
       "      <td>2332</td>\n",
       "      <td>5858.0</td>\n",
       "      <td>5707.7</td>\n",
       "      <td>21860</td>\n",
       "      <td>1855</td>\n",
       "      <td>6336.3</td>\n",
       "      <td>5577.9</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.632900e-08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32237</th>\n",
       "      <td>21861</td>\n",
       "      <td>2332</td>\n",
       "      <td>5872.1</td>\n",
       "      <td>5709.3</td>\n",
       "      <td>21861</td>\n",
       "      <td>1855</td>\n",
       "      <td>6336.3</td>\n",
       "      <td>5577.9</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.149400e-08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32248</th>\n",
       "      <td>21862</td>\n",
       "      <td>2332</td>\n",
       "      <td>5885.9</td>\n",
       "      <td>5710.9</td>\n",
       "      <td>21862</td>\n",
       "      <td>1855</td>\n",
       "      <td>6336.3</td>\n",
       "      <td>5577.9</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.062600e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32262</th>\n",
       "      <td>21863</td>\n",
       "      <td>2332</td>\n",
       "      <td>5899.5</td>\n",
       "      <td>5712.7</td>\n",
       "      <td>21863</td>\n",
       "      <td>1855</td>\n",
       "      <td>6336.3</td>\n",
       "      <td>5577.9</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.090200e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32274</th>\n",
       "      <td>21864</td>\n",
       "      <td>2332</td>\n",
       "      <td>5912.9</td>\n",
       "      <td>5714.4</td>\n",
       "      <td>21864</td>\n",
       "      <td>1855</td>\n",
       "      <td>6336.3</td>\n",
       "      <td>5577.9</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.350600e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32286</th>\n",
       "      <td>21865</td>\n",
       "      <td>2332</td>\n",
       "      <td>5926.1</td>\n",
       "      <td>5716.1</td>\n",
       "      <td>21865</td>\n",
       "      <td>1855</td>\n",
       "      <td>6336.3</td>\n",
       "      <td>5577.9</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.636500e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32314</th>\n",
       "      <td>21868</td>\n",
       "      <td>2332</td>\n",
       "      <td>5965.4</td>\n",
       "      <td>5720.9</td>\n",
       "      <td>21868</td>\n",
       "      <td>1855</td>\n",
       "      <td>6336.3</td>\n",
       "      <td>5577.9</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.546000e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32324</th>\n",
       "      <td>21869</td>\n",
       "      <td>2332</td>\n",
       "      <td>5978.3</td>\n",
       "      <td>5722.0</td>\n",
       "      <td>21869</td>\n",
       "      <td>1855</td>\n",
       "      <td>6336.3</td>\n",
       "      <td>5577.9</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.049000e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32338</th>\n",
       "      <td>21870</td>\n",
       "      <td>2332</td>\n",
       "      <td>5991.3</td>\n",
       "      <td>5723.1</td>\n",
       "      <td>21870</td>\n",
       "      <td>1855</td>\n",
       "      <td>6336.3</td>\n",
       "      <td>5577.9</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.413400e-08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32354</th>\n",
       "      <td>21871</td>\n",
       "      <td>2332</td>\n",
       "      <td>6004.9</td>\n",
       "      <td>5727.6</td>\n",
       "      <td>21871</td>\n",
       "      <td>1855</td>\n",
       "      <td>6336.8</td>\n",
       "      <td>5577.9</td>\n",
       "      <td>1.62270</td>\n",
       "      <td>0.125860</td>\n",
       "      <td>1.002500e-07</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32366</th>\n",
       "      <td>21872</td>\n",
       "      <td>2332</td>\n",
       "      <td>6020.4</td>\n",
       "      <td>5728.9</td>\n",
       "      <td>21872</td>\n",
       "      <td>1855</td>\n",
       "      <td>6339.7</td>\n",
       "      <td>5577.7</td>\n",
       "      <td>3.61390</td>\n",
       "      <td>-0.070982</td>\n",
       "      <td>2.556800e-08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32379</th>\n",
       "      <td>21873</td>\n",
       "      <td>2332</td>\n",
       "      <td>6036.1</td>\n",
       "      <td>5730.3</td>\n",
       "      <td>21873</td>\n",
       "      <td>1855</td>\n",
       "      <td>6344.4</td>\n",
       "      <td>5575.9</td>\n",
       "      <td>5.88760</td>\n",
       "      <td>-2.562000</td>\n",
       "      <td>1.773400e-08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32393</th>\n",
       "      <td>21874</td>\n",
       "      <td>2332</td>\n",
       "      <td>6051.7</td>\n",
       "      <td>5731.7</td>\n",
       "      <td>21874</td>\n",
       "      <td>1855</td>\n",
       "      <td>6347.5</td>\n",
       "      <td>5569.2</td>\n",
       "      <td>2.43880</td>\n",
       "      <td>-8.502300</td>\n",
       "      <td>5.663400e-08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32404</th>\n",
       "      <td>21875</td>\n",
       "      <td>2332</td>\n",
       "      <td>6067.4</td>\n",
       "      <td>5733.0</td>\n",
       "      <td>21875</td>\n",
       "      <td>1855</td>\n",
       "      <td>6350.6</td>\n",
       "      <td>5558.9</td>\n",
       "      <td>-0.75284</td>\n",
       "      <td>-10.977000</td>\n",
       "      <td>2.600700e-07</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32417</th>\n",
       "      <td>21876</td>\n",
       "      <td>2332</td>\n",
       "      <td>6083.1</td>\n",
       "      <td>5734.3</td>\n",
       "      <td>21876</td>\n",
       "      <td>1855</td>\n",
       "      <td>6349.8</td>\n",
       "      <td>5546.7</td>\n",
       "      <td>-0.89740</td>\n",
       "      <td>-13.085000</td>\n",
       "      <td>2.095100e-07</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32429</th>\n",
       "      <td>21877</td>\n",
       "      <td>2332</td>\n",
       "      <td>6098.9</td>\n",
       "      <td>5735.4</td>\n",
       "      <td>21877</td>\n",
       "      <td>1855</td>\n",
       "      <td>6348.8</td>\n",
       "      <td>5532.3</td>\n",
       "      <td>-1.02520</td>\n",
       "      <td>-14.948000</td>\n",
       "      <td>2.103400e-07</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32442</th>\n",
       "      <td>21878</td>\n",
       "      <td>2332</td>\n",
       "      <td>6114.6</td>\n",
       "      <td>5736.3</td>\n",
       "      <td>21878</td>\n",
       "      <td>1855</td>\n",
       "      <td>6347.8</td>\n",
       "      <td>5517.3</td>\n",
       "      <td>-1.02790</td>\n",
       "      <td>-14.988000</td>\n",
       "      <td>3.831500e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32453</th>\n",
       "      <td>21879</td>\n",
       "      <td>2332</td>\n",
       "      <td>6130.3</td>\n",
       "      <td>5737.2</td>\n",
       "      <td>21879</td>\n",
       "      <td>1855</td>\n",
       "      <td>6346.8</td>\n",
       "      <td>5502.3</td>\n",
       "      <td>-1.02510</td>\n",
       "      <td>-14.947000</td>\n",
       "      <td>1.155000e-08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32465</th>\n",
       "      <td>21880</td>\n",
       "      <td>2332</td>\n",
       "      <td>6146.0</td>\n",
       "      <td>5738.0</td>\n",
       "      <td>21880</td>\n",
       "      <td>1855</td>\n",
       "      <td>6345.8</td>\n",
       "      <td>5487.7</td>\n",
       "      <td>-0.92464</td>\n",
       "      <td>-13.482000</td>\n",
       "      <td>3.881600e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       re_time  re_ID    re_x    re_y  tr_time  tr_ID    tr_x    tr_y  \\\n",
       "32073    21846   2332  5594.9  5677.9    21846   1855  6336.3  5577.9   \n",
       "32090    21848   2332  5638.7  5683.5    21848   1855  6336.3  5577.9   \n",
       "32106    21850   2332  5682.5  5689.1    21850   1855  6336.3  5577.9   \n",
       "32113    21851   2332  5704.4  5691.9    21851   1855  6336.3  5577.9   \n",
       "32124    21852   2332  5726.1  5694.7    21852   1855  6336.3  5577.9   \n",
       "32150    21854   2332  5764.3  5699.6    21854   1855  6336.3  5577.9   \n",
       "32189    21857   2332  5813.0  5704.2    21857   1855  6336.3  5577.9   \n",
       "32199    21858   2332  5828.5  5705.3    21858   1855  6336.3  5577.9   \n",
       "32212    21859   2332  5843.5  5706.4    21859   1855  6336.3  5577.9   \n",
       "32225    21860   2332  5858.0  5707.7    21860   1855  6336.3  5577.9   \n",
       "32237    21861   2332  5872.1  5709.3    21861   1855  6336.3  5577.9   \n",
       "32248    21862   2332  5885.9  5710.9    21862   1855  6336.3  5577.9   \n",
       "32262    21863   2332  5899.5  5712.7    21863   1855  6336.3  5577.9   \n",
       "32274    21864   2332  5912.9  5714.4    21864   1855  6336.3  5577.9   \n",
       "32286    21865   2332  5926.1  5716.1    21865   1855  6336.3  5577.9   \n",
       "32314    21868   2332  5965.4  5720.9    21868   1855  6336.3  5577.9   \n",
       "32324    21869   2332  5978.3  5722.0    21869   1855  6336.3  5577.9   \n",
       "32338    21870   2332  5991.3  5723.1    21870   1855  6336.3  5577.9   \n",
       "32354    21871   2332  6004.9  5727.6    21871   1855  6336.8  5577.9   \n",
       "32366    21872   2332  6020.4  5728.9    21872   1855  6339.7  5577.7   \n",
       "32379    21873   2332  6036.1  5730.3    21873   1855  6344.4  5575.9   \n",
       "32393    21874   2332  6051.7  5731.7    21874   1855  6347.5  5569.2   \n",
       "32404    21875   2332  6067.4  5733.0    21875   1855  6350.6  5558.9   \n",
       "32417    21876   2332  6083.1  5734.3    21876   1855  6349.8  5546.7   \n",
       "32429    21877   2332  6098.9  5735.4    21877   1855  6348.8  5532.3   \n",
       "32442    21878   2332  6114.6  5736.3    21878   1855  6347.8  5517.3   \n",
       "32453    21879   2332  6130.3  5737.2    21879   1855  6346.8  5502.3   \n",
       "32465    21880   2332  6146.0  5738.0    21880   1855  6345.8  5487.7   \n",
       "\n",
       "         tr_vx      tr_vy          RSSI  Label  \n",
       "32073  0.00000   0.000000  2.503500e-09    1.0  \n",
       "32090  0.00000   0.000000  2.405400e-09    1.0  \n",
       "32106  0.00000   0.000000  4.292200e-09    1.0  \n",
       "32113  0.00000   0.000000  1.367800e-09    1.0  \n",
       "32124  0.00000   0.000000  1.697000e-09    1.0  \n",
       "32150  0.00000   0.000000  2.217600e-09    1.0  \n",
       "32189  0.00000   0.000000  1.758700e-09    1.0  \n",
       "32199  0.00000   0.000000  4.445700e-09    1.0  \n",
       "32212  0.00000   0.000000  2.930300e-09    1.0  \n",
       "32225  0.00000   0.000000  2.632900e-08    1.0  \n",
       "32237  0.00000   0.000000  1.149400e-08    1.0  \n",
       "32248  0.00000   0.000000  6.062600e-09    1.0  \n",
       "32262  0.00000   0.000000  8.090200e-09    1.0  \n",
       "32274  0.00000   0.000000  5.350600e-09    1.0  \n",
       "32286  0.00000   0.000000  1.636500e-09    1.0  \n",
       "32314  0.00000   0.000000  8.546000e-09    1.0  \n",
       "32324  0.00000   0.000000  9.049000e-09    1.0  \n",
       "32338  0.00000   0.000000  6.413400e-08    1.0  \n",
       "32354  1.62270   0.125860  1.002500e-07    1.0  \n",
       "32366  3.61390  -0.070982  2.556800e-08    1.0  \n",
       "32379  5.88760  -2.562000  1.773400e-08    1.0  \n",
       "32393  2.43880  -8.502300  5.663400e-08    1.0  \n",
       "32404 -0.75284 -10.977000  2.600700e-07    1.0  \n",
       "32417 -0.89740 -13.085000  2.095100e-07    1.0  \n",
       "32429 -1.02520 -14.948000  2.103400e-07    1.0  \n",
       "32442 -1.02790 -14.988000  3.831500e-09    1.0  \n",
       "32453 -1.02510 -14.947000  1.155000e-08    1.0  \n",
       "32465 -0.92464 -13.482000  3.881600e-09    1.0  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_select = get_data(data_atk2, 1)\n",
    "data_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The detection system said: The BSM is normal!\n",
      "But in fact, the BSM is malicious, the detection is incorrect\n"
     ]
    }
   ],
   "source": [
    "test_real_data(model2, data_select, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
